{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Uninstall potentially conflicting packages\n",
    "!pip uninstall -y transformers accelerate unsloth torch torchvision torchaudio\n",
    "\n",
    "# Install base packages\n",
    "!pip install unsloth\n",
    "\n",
    "# Install dependencies\n",
    "!pip install -q transformers accelerate peft\n",
    "!pip install -q datasets evaluate bitsandbytes trl\n",
    "!pip install -q torch torchvision torchaudio\n",
    "\n",
    "# Install Colab-optimized unsloth\n",
    "!pip uninstall unsloth -y\n",
    "!pip install --upgrade --no-cache-dir \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "\n",
    "# Install other tools\n",
    "!pip install pandas scikit-learn\n",
    "!pip install -q ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import gc\n",
    "from datasets import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from unsloth import FastLanguageModel\n",
    "from peft import PeftModel\n",
    "import warnings\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Data and model paths\n",
    "data_dir = \"/content/drive/MyDrive/ai_dataset\"\n",
    "save_dir = \"/content/drive/MyDrive/ai_detection_model\"\n",
    "final_model_path = os.path.join(save_dir, \"final_model\")\n",
    "\n",
    "# Fix random seeds for reproducibility\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "def set_seeds(seed=3407):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seeds()\n",
    "\n",
    "def clear_memory():\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "clear_memory()\n",
    "torch.set_float32_matmul_precision('high')\n",
    "\n",
    "# ===================== Dataset Preparation =====================\n",
    "human_xlsx_path = os.path.join(data_dir, \"human.xlsx\")\n",
    "ai_xlsx_path = os.path.join(data_dir, \"ai.xlsx\")\n",
    "\n",
    "df_human = pd.read_excel(human_xlsx_path)\n",
    "df_human = df_human[['abstract']]\n",
    "df_human['is_ai_generated'] = 'False'\n",
    "\n",
    "df_ai = pd.read_excel(ai_xlsx_path)\n",
    "df_ai = df_ai[['abstract']]\n",
    "df_ai['is_ai_generated'] = 'True'\n",
    "\n",
    "df = pd.concat([df_human, df_ai], ignore_index=True)\n",
    "dataset = Dataset.from_pandas(df)\n",
    "\n",
    "# Limit the dataset sample size, e.g., using 1000 samples for testing\n",
    "max_samples = 1000\n",
    "if max_samples > 0 and max_samples < len(dataset):\n",
    "    dataset = dataset.shuffle(seed=3407).select(range(min(max_samples, len(dataset))))\n",
    "\n",
    "train_val_idx, test_idx = train_test_split(\n",
    "    range(len(dataset)),\n",
    "    test_size=0.2,\n",
    "    random_state=3407\n",
    ")\n",
    "\n",
    "train_idx, val_idx = train_test_split(\n",
    "    train_val_idx,\n",
    "    test_size=0.125, # 0.125 * 0.8 = 0.1 (final 70%:10%:20%)\n",
    "    random_state=3407\n",
    ")\n",
    "\n",
    "def process_training_example(example):\n",
    "    text = example['abstract']\n",
    "    is_ai_generated = example['is_ai_generated']\n",
    "    prompt = (\n",
    "        \"You are an expert in distinguishing between text written by humans and text generated by AI.\\n\\n\"\n",
    "        f\"Given Text: {text}\\n\\n\"\n",
    "        \"Based on careful analysis, is the text generated by an AI? Respond with EXACTLY 'True' or 'False'.\\n\"\n",
    "        f\"Answer: {str(is_ai_generated)}\"\n",
    "    )\n",
    "    return {\"text\": prompt}\n",
    "\n",
    "def process_test_example(example):\n",
    "    text = example['abstract']\n",
    "    prompt = (\n",
    "        \"You are an expert in distinguishing between text written by humans and text generated by AI.\\n\\n\"\n",
    "        f\"Given Text: {text}\\n\\n\"\n",
    "        \"Based on careful analysis, is the text generated by an AI? Respond with EXACTLY 'True' or 'False'.\\n\"\n",
    "        \"Answer:\"\n",
    "    )\n",
    "    return prompt\n",
    "\n",
    "train_examples = [process_training_example(dataset[i]) for i in train_idx]\n",
    "eval_examples = [process_training_example(dataset[i]) for i in val_idx]\n",
    "test_examples = [dataset[i] for i in test_idx]\n",
    "\n",
    "train_dataset = Dataset.from_list(train_examples)\n",
    "eval_dataset = Dataset.from_list(eval_examples)\n",
    "\n",
    "# Keep true labels in test_dataset for evaluating predictions\n",
    "test_dataset = Dataset.from_list([\n",
    "    {'abstract': item['abstract'], 'is_ai_generated': item['is_ai_generated']} for item in test_examples\n",
    "])\n",
    "\n",
    "print(f\"Train size: {len(train_dataset)}, Eval size: {len(eval_dataset)}, Test size: {len(test_dataset)}\")\n",
    "\n",
    "# ===================== Load Final Model =====================\n",
    "base_model_name = \"unsloth/Meta-Llama-3.1-8B\"  # Same base model as during training\n",
    "\n",
    "clear_memory()\n",
    "\n",
    "base_model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=base_model_name,\n",
    "    max_seq_length=2048,\n",
    "    load_in_4bit=True,\n",
    ")\n",
    "\n",
    "model = PeftModel.from_pretrained(base_model, final_model_path)\n",
    "model.eval()\n",
    "\n",
    "# Enter inference mode\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "def parse_prediction(response: str) -> bool:\n",
    "    response = response.lower().strip()\n",
    "    if \"true\" in response and \"false\" not in response:\n",
    "        return True   # Predicted as AI generated\n",
    "    elif \"false\" in response and \"true\" not in response:\n",
    "        return False  # Predicted as Human generated\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "# ===================== Inference and Evaluation on Test Set =====================\n",
    "batch_size = 16\n",
    "test_examples_list = list(test_dataset)\n",
    "total_batches = (len(test_examples_list) + batch_size - 1) // batch_size\n",
    "all_predictions = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(0, len(test_examples_list), batch_size):\n",
    "        if i % (batch_size * 10) == 0:\n",
    "            print(f\"Processing batch {i//batch_size}/{total_batches}\")\n",
    "\n",
    "        batch = test_examples_list[i:i + batch_size]\n",
    "        prompts = [process_test_example(example) for example in batch]\n",
    "\n",
    "        inputs = tokenizer(\n",
    "            prompts,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=2048\n",
    "        ).to(\"cuda\")\n",
    "\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=8,\n",
    "            temperature=0.1,\n",
    "            top_p=0.9,\n",
    "            do_sample=False,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "        )\n",
    "\n",
    "        input_length = inputs['input_ids'].shape[1]\n",
    "        responses = tokenizer.batch_decode(\n",
    "            [output[input_length:] for output in outputs],\n",
    "            skip_special_tokens=True\n",
    "        )\n",
    "\n",
    "        batch_predictions = [parse_prediction(response) for response in responses]\n",
    "        all_predictions.extend(batch_predictions)\n",
    "\n",
    "print(f\"Total predictions: {len(all_predictions)}\")\n",
    "\n",
    "# Convert true labels to boolean, True for AI generated, False for Human\n",
    "true_labels = [row['is_ai_generated'] == 'True' for row in test_dataset]\n",
    "\n",
    "# Save predictions and true labels to CSV, include correctness column\n",
    "submission_df = pd.DataFrame({\n",
    "    'ID': range(len(all_predictions)),\n",
    "    'is_ai_generated_pred': all_predictions,\n",
    "    'is_ai_generated_true': true_labels\n",
    "})\n",
    "\n",
    "submission_df['correctness'] = (submission_df['is_ai_generated_pred'] == submission_df['is_ai_generated_true'])\n",
    "\n",
    "submission_path = os.path.join(save_dir, 'submission_with_truth.csv')\n",
    "submission_df.to_csv(submission_path, index=False)\n",
    "print(f\"Submission with truth saved to {submission_path}\")\n",
    "\n",
    "print(submission_df.head())\n",
    "\n",
    "# Output classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(true_labels, all_predictions, target_names=[\"Human\",\"AI\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "max_samples = 5000\n",
    "\n",
    "human_xlsx_path = os.path.join(data_dir, \"human.xlsx\")\n",
    "ai_xlsx_path = os.path.join(data_dir, \"ai.xlsx\")\n",
    "\n",
    "df_human = pd.read_excel(human_xlsx_path)\n",
    "df_human = df_human[['abstract']]\n",
    "df_human['is_ai_generated'] = 'False'\n",
    "\n",
    "df_ai = pd.read_excel(ai_xlsx_path)\n",
    "df_ai = df_ai[['abstract']]\n",
    "df_ai['is_ai_generated'] = 'True'\n",
    "\n",
    "df = pd.concat([df_human, df_ai], ignore_index=True)\n",
    "dataset = Dataset.from_pandas(df)\n",
    "\n",
    "if max_samples > 0 and max_samples < len(dataset):\n",
    "    dataset = dataset.shuffle(seed=3407).select(range(min(max_samples, len(dataset))))\n",
    "\n",
    "train_val_idx, test_idx = train_test_split(\n",
    "    range(len(dataset)),\n",
    "    test_size=0.2,\n",
    "    random_state=3407\n",
    ")\n",
    "\n",
    "train_idx, val_idx = train_test_split(\n",
    "    train_val_idx,\n",
    "    test_size=0.125, # 0.125 * 0.8 = 0.1\n",
    "    random_state=3407\n",
    ")\n",
    "\n",
    "def process_training_example(example):\n",
    "    text = example['abstract']\n",
    "    is_ai_generated = example['is_ai_generated']\n",
    "    prompt = (\n",
    "        \"You are an expert in distinguishing between text written by humans and text generated by AI.\\n\\n\"\n",
    "        f\"Given Text: {text}\\n\\n\"\n",
    "        \"Based on careful analysis, is the text generated by an AI? Respond with EXACTLY 'True' or 'False'.\\n\"\n",
    "        f\"Answer: {str(is_ai_generated)}\"\n",
    "    )\n",
    "    return {\"text\": prompt}\n",
    "\n",
    "def process_test_example(example):\n",
    "    text = example['abstract']\n",
    "    prompt = (\n",
    "        \"You are an expert in distinguishing between text written by humans and text generated by AI.\\n\\n\"\n",
    "        f\"Given Text: {text}\\n\\n\"\n",
    "        \"Based on careful analysis, is the text generated by an AI? Respond with EXACTLY 'True' or 'False'.\\n\"\n",
    "        \"Answer:\"\n",
    "    )\n",
    "    return prompt\n",
    "\n",
    "train_examples = [process_training_example(dataset[i]) for i in train_idx]\n",
    "eval_examples = [process_training_example(dataset[i]) for i in val_idx]\n",
    "test_examples = [dataset[i] for i in test_idx]\n",
    "\n",
    "train_dataset = Dataset.from_list(train_examples)\n",
    "eval_dataset = Dataset.from_list(eval_examples)\n",
    "test_dataset = Dataset.from_list([\n",
    "    {'abstract': item['abstract'], 'is_ai_generated': item['is_ai_generated']} for item in test_examples\n",
    "])\n",
    "\n",
    "print(f\"Train size: {len(train_dataset)}, Eval size: {len(eval_dataset)}, Test size: {len(test_dataset)}\")\n",
    "\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "batch_size = 16\n",
    "test_examples_list = list(test_dataset)\n",
    "total_batches = (len(test_examples_list) + batch_size - 1) // batch_size\n",
    "all_predictions = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(0, len(test_examples_list), batch_size):\n",
    "        if i % (batch_size * 10) == 0:\n",
    "            print(f\"Processing batch {i//batch_size}/{total_batches}\")\n",
    "\n",
    "        batch = test_examples_list[i:i + batch_size]\n",
    "        prompts = [process_test_example(example) for example in batch]\n",
    "\n",
    "        inputs = tokenizer(\n",
    "            prompts,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=2048\n",
    "        ).to(\"cuda\")\n",
    "\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=8,\n",
    "            temperature=0.1,\n",
    "            top_p=0.9,\n",
    "            do_sample=False,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "        )\n",
    "\n",
    "        input_length = inputs['input_ids'].shape[1]\n",
    "        responses = tokenizer.batch_decode(\n",
    "            [output[input_length:] for output in outputs],\n",
    "            skip_special_tokens=True\n",
    "        )\n",
    "\n",
    "        batch_predictions = [parse_prediction(response) for response in responses]\n",
    "        all_predictions.extend(batch_predictions)\n",
    "\n",
    "print(f\"Total predictions: {len(all_predictions)}\")\n",
    "\n",
    "true_labels = [row['is_ai_generated'] == 'True' for row in test_dataset]\n",
    "\n",
    "submission_df = pd.DataFrame({\n",
    "    'ID': range(len(all_predictions)),\n",
    "    'is_ai_generated_pred': all_predictions,\n",
    "    'is_ai_generated_true': true_labels\n",
    "})\n",
    "\n",
    "submission_df['correctness'] = (submission_df['is_ai_generated_pred'] == submission_df['is_ai_generated_true'])\n",
    "\n",
    "submission_path = os.path.join(save_dir, 'submission_with_truth_5000.csv')\n",
    "submission_df.to_csv(submission_path, index=False)\n",
    "print(f\"Submission with truth saved to {submission_path}\")\n",
    "\n",
    "print(submission_df.head())\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(true_labels, all_predictions, target_names=[\"Human\",\"AI\"]))\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
