{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bnbryan/dl-final-proj/blob/failed/NYU_DL_24_Final_lc5464.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "QKxfGTnfq2Sn",
        "outputId": "f80a280e-6e6b-45d8-c537-57a31222762f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: transformers 4.47.0\n",
            "Uninstalling transformers-4.47.0:\n",
            "  Successfully uninstalled transformers-4.47.0\n",
            "Found existing installation: accelerate 1.2.1\n",
            "Uninstalling accelerate-1.2.1:\n",
            "  Successfully uninstalled accelerate-1.2.1\n",
            "Found existing installation: unsloth 2024.12.4\n",
            "Uninstalling unsloth-2024.12.4:\n",
            "  Successfully uninstalled unsloth-2024.12.4\n",
            "Found existing installation: torch 2.5.1\n",
            "Uninstalling torch-2.5.1:\n",
            "  Successfully uninstalled torch-2.5.1\n",
            "Found existing installation: torchvision 0.20.1\n",
            "Uninstalling torchvision-0.20.1:\n",
            "  Successfully uninstalled torchvision-0.20.1\n",
            "Found existing installation: torchaudio 2.5.1\n",
            "Uninstalling torchaudio-2.5.1:\n",
            "  Successfully uninstalled torchaudio-2.5.1\n",
            "Collecting unsloth\n",
            "  Using cached unsloth-2024.12.4-py3-none-any.whl.metadata (59 kB)\n",
            "Requirement already satisfied: unsloth_zoo>=2024.11.8 in /usr/local/lib/python3.10/dist-packages (from unsloth) (2024.12.1)\n",
            "Collecting torch>=2.4.0 (from unsloth)\n",
            "  Using cached torch-2.5.1-cp310-cp310-manylinux1_x86_64.whl.metadata (28 kB)\n",
            "Requirement already satisfied: xformers>=0.0.27.post2 in /usr/local/lib/python3.10/dist-packages (from unsloth) (0.0.28.post3)\n",
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.10/dist-packages (from unsloth) (0.45.0)\n",
            "Requirement already satisfied: triton>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from unsloth) (3.1.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from unsloth) (24.2)\n",
            "Requirement already satisfied: tyro in /usr/local/lib/python3.10/dist-packages (from unsloth) (0.9.2)\n",
            "Collecting transformers>=4.46.1 (from unsloth)\n",
            "  Using cached transformers-4.47.0-py3-none-any.whl.metadata (43 kB)\n",
            "Requirement already satisfied: datasets>=2.16.0 in /usr/local/lib/python3.10/dist-packages (from unsloth) (3.2.0)\n",
            "Requirement already satisfied: sentencepiece>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from unsloth) (0.2.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from unsloth) (4.66.6)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from unsloth) (5.9.5)\n",
            "Requirement already satisfied: wheel>=0.42.0 in /usr/local/lib/python3.10/dist-packages (from unsloth) (0.45.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from unsloth) (1.26.4)\n",
            "Collecting accelerate>=0.34.1 (from unsloth)\n",
            "  Using cached accelerate-1.2.1-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: trl!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,>=0.7.9 in /usr/local/lib/python3.10/dist-packages (from unsloth) (0.13.0)\n",
            "Requirement already satisfied: peft!=0.11.0,>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from unsloth) (0.13.2)\n",
            "Requirement already satisfied: protobuf<4.0.0 in /usr/local/lib/python3.10/dist-packages (from unsloth) (3.20.3)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (from unsloth) (0.26.5)\n",
            "Requirement already satisfied: hf_transfer in /usr/local/lib/python3.10/dist-packages (from unsloth) (0.1.8)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.34.1->unsloth) (6.0.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.34.1->unsloth) (0.4.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth) (3.16.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth) (2.32.3)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets>=2.16.0->unsloth) (2024.9.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth) (3.11.10)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->unsloth) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.4.0->unsloth) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.4.0->unsloth) (3.1.4)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.10/dist-packages (from torch>=2.4.0->unsloth) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.10/dist-packages (from torch>=2.4.0->unsloth) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.10/dist-packages (from torch>=2.4.0->unsloth) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.10/dist-packages (from torch>=2.4.0->unsloth) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.10/dist-packages (from torch>=2.4.0->unsloth) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.10/dist-packages (from torch>=2.4.0->unsloth) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.10/dist-packages (from torch>=2.4.0->unsloth) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.10/dist-packages (from torch>=2.4.0->unsloth) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.10/dist-packages (from torch>=2.4.0->unsloth) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.10/dist-packages (from torch>=2.4.0->unsloth) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.10/dist-packages (from torch>=2.4.0->unsloth) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.10/dist-packages (from torch>=2.4.0->unsloth) (12.4.127)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=2.4.0->unsloth) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=2.4.0->unsloth) (1.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.46.1->unsloth) (2024.9.11)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.46.1->unsloth) (0.21.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from trl!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,>=0.7.9->unsloth) (13.9.4)\n",
            "Requirement already satisfied: cut_cross_entropy in /usr/local/lib/python3.10/dist-packages (from unsloth_zoo>=2024.11.8->unsloth) (24.12.2)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from unsloth_zoo>=2024.11.8->unsloth) (11.0.0)\n",
            "Requirement already satisfied: docstring-parser>=0.16 in /usr/local/lib/python3.10/dist-packages (from tyro->unsloth) (0.16)\n",
            "Requirement already satisfied: shtab>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from tyro->unsloth) (1.7.1)\n",
            "Requirement already satisfied: typeguard>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from tyro->unsloth) (4.4.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->unsloth) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->unsloth) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->unsloth) (4.0.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->unsloth) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->unsloth) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->unsloth) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->unsloth) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->unsloth) (1.18.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth) (2024.8.30)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->trl!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,>=0.7.9->unsloth) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->trl!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,>=0.7.9->unsloth) (2.18.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2.4.0->unsloth) (3.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=2.16.0->unsloth) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=2.16.0->unsloth) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=2.16.0->unsloth) (2024.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->trl!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,>=0.7.9->unsloth) (0.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets>=2.16.0->unsloth) (1.17.0)\n",
            "Using cached unsloth-2024.12.4-py3-none-any.whl (174 kB)\n",
            "Using cached accelerate-1.2.1-py3-none-any.whl (336 kB)\n",
            "Using cached torch-2.5.1-cp310-cp310-manylinux1_x86_64.whl (906.4 MB)\n",
            "Using cached transformers-4.47.0-py3-none-any.whl (10.1 MB)\n",
            "Installing collected packages: torch, transformers, accelerate, unsloth\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "fastai 2.7.18 requires torchvision>=0.11, which is not installed.\n",
            "timm 1.0.12 requires torchvision, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed accelerate-1.2.1 torch-2.5.1 transformers-4.47.0 unsloth-2024.12.4\n",
            "Found existing installation: unsloth 2024.12.4\n",
            "Uninstalling unsloth-2024.12.4:\n",
            "  Successfully uninstalled unsloth-2024.12.4\n",
            "Collecting unsloth@ git+https://github.com/unslothai/unsloth.git (from unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
            "  Cloning https://github.com/unslothai/unsloth.git to /tmp/pip-install-z83s1bnk/unsloth_a5ec2ca1341340f8bd604140c0f285a4\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/unslothai/unsloth.git /tmp/pip-install-z83s1bnk/unsloth_a5ec2ca1341340f8bd604140c0f285a4\n",
            "  Resolved https://github.com/unslothai/unsloth.git to commit 85f1fa096afde5efe2fb8521d8ceec8d13a00715\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: unsloth_zoo>=2024.11.8 in /usr/local/lib/python3.10/dist-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2024.12.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (24.2)\n",
            "Requirement already satisfied: tyro in /usr/local/lib/python3.10/dist-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.9.2)\n",
            "Requirement already satisfied: transformers>=4.46.1 in /usr/local/lib/python3.10/dist-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (4.47.0)\n",
            "Requirement already satisfied: datasets>=2.16.0 in /usr/local/lib/python3.10/dist-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.2.0)\n",
            "Requirement already satisfied: sentencepiece>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.2.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (4.66.6)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (5.9.5)\n",
            "Requirement already satisfied: wheel>=0.42.0 in /usr/local/lib/python3.10/dist-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.45.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.26.4)\n",
            "Requirement already satisfied: protobuf<4.0.0 in /usr/local/lib/python3.10/dist-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.20.3)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.26.5)\n",
            "Requirement already satisfied: hf_transfer in /usr/local/lib/python3.10/dist-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.1.8)\n",
            "Requirement already satisfied: bitsandbytes>=0.43.3 in /usr/local/lib/python3.10/dist-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.45.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from bitsandbytes>=0.43.3->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.5.1)\n",
            "Requirement already satisfied: typing_extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from bitsandbytes>=0.43.3->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (4.12.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.16.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.32.3)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2024.9.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.11.10)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.46.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2024.9.11)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.46.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.46.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.4.5)\n",
            "Requirement already satisfied: triton in /usr/local/lib/python3.10/dist-packages (from unsloth_zoo>=2024.11.8->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.1.0)\n",
            "Requirement already satisfied: accelerate>=0.34.1 in /usr/local/lib/python3.10/dist-packages (from unsloth_zoo>=2024.11.8->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.2.1)\n",
            "Requirement already satisfied: trl!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,>=0.7.9 in /usr/local/lib/python3.10/dist-packages (from unsloth_zoo>=2024.11.8->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.13.0)\n",
            "Requirement already satisfied: peft!=0.11.0,>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from unsloth_zoo>=2024.11.8->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.13.2)\n",
            "Requirement already satisfied: cut_cross_entropy in /usr/local/lib/python3.10/dist-packages (from unsloth_zoo>=2024.11.8->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (24.12.2)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from unsloth_zoo>=2024.11.8->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (11.0.0)\n",
            "Requirement already satisfied: docstring-parser>=0.16 in /usr/local/lib/python3.10/dist-packages (from tyro->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.16)\n",
            "Requirement already satisfied: rich>=11.1.0 in /usr/local/lib/python3.10/dist-packages (from tyro->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (13.9.4)\n",
            "Requirement already satisfied: shtab>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from tyro->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.7.1)\n",
            "Requirement already satisfied: typeguard>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from tyro->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (4.4.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (4.0.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.18.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2024.8.30)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1.0->tyro->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1.0->tyro->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.18.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes>=0.43.3->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes>=0.43.3->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.1.4)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes>=0.43.3->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes>=0.43.3->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes>=0.43.3->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes>=0.43.3->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes>=0.43.3->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes>=0.43.3->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes>=0.43.3->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes>=0.43.3->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes>=0.43.3->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes>=0.43.3->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes>=0.43.3->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes>=0.43.3->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (12.4.127)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes>=0.43.3->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch->bitsandbytes>=0.43.3->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.3.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2024.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->bitsandbytes>=0.43.3->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.0.2)\n",
            "Building wheels for collected packages: unsloth\n",
            "  Building wheel for unsloth (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for unsloth: filename=unsloth-2024.12.4-py3-none-any.whl size=173746 sha256=406e115653ea03ffbaffde0695e9ecb1afd47be092375a4a5af63a25a50b5dbf\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-8vcd7au3/wheels/ed/d4/e9/76fb290ee3df0a5fc21ce5c2c788e29e9607a2353d8342fd0d\n",
            "Successfully built unsloth\n",
            "Installing collected packages: unsloth\n",
            "Successfully installed unsloth-2024.12.4\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.5.2)\n",
            "Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "# Uninstall potentially conflicting packages\n",
        "!pip uninstall -y transformers accelerate unsloth torch torchvision torchaudio\n",
        "\n",
        "# Install base packages\n",
        "!pip install unsloth\n",
        "\n",
        "# Install dependencies\n",
        "!pip install -q transformers accelerate peft\n",
        "!pip install -q datasets evaluate bitsandbytes trl\n",
        "!pip install -q torch torchvision torchaudio\n",
        "\n",
        "# Install Colab-optimized unsloth\n",
        "!pip uninstall unsloth -y\n",
        "!pip install --upgrade --no-cache-dir \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
        "\n",
        "# Install other tools\n",
        "!pip install pandas scikit-learn\n",
        "!pip install -q ipywidgets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uIWN2AazbCtN"
      },
      "source": [
        "Preparations\n",
        "------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "q2ADdar0bCBw",
        "outputId": "05699410-8790-42e5-c54f-e4027564e5a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
            "ü¶• Unsloth Zoo will now patch everything to make training faster!\n",
            "PyTorch version: 2.5.1+cu124\n",
            "Transformers version: 4.47.0\n",
            "Accelerate version: 1.2.1\n"
          ]
        }
      ],
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Environment setup\n",
        "import os\n",
        "import warnings\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "from datasets import load_dataset, Dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "import gc\n",
        "from unsloth import FastLanguageModel\n",
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments, TrainerCallback\n",
        "import transformers\n",
        "import accelerate\n",
        "import json\n",
        "import openpyxl\n",
        "from datasets import Dataset\n",
        "import gdown\n",
        "\n",
        "# Print versions\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"Transformers version: {transformers.__version__}\")\n",
        "print(f\"Accelerate version: {accelerate.__version__}\")\n",
        "\n",
        "# Configure environment\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
        "warnings.filterwarnings('ignore')\n",
        "torch.set_float32_matmul_precision('high')\n",
        "\n",
        "# Set random seeds\n",
        "def set_seeds(seed=3407):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "# Memory management utilities\n",
        "def clear_memory():\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "def print_gpu_utilization():\n",
        "    print(\"\\nGPU Memory Usage:\")\n",
        "    !nvidia-smi | grep -E \"Memory|Volatile\"\n",
        "\n",
        "def print_detailed_gpu_info():\n",
        "    print(\"\\nDetailed GPU Memory Info:\")\n",
        "    print(f\"Allocated: {torch.cuda.memory_allocated()/1024**2:.2f} MB\")\n",
        "    print(f\"Cached: {torch.cuda.memory_reserved()/1024**2:.2f} MB\")\n",
        "    print(f\"Max Allocated: {torch.cuda.max_memory_allocated()/1024**2:.2f} MB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bBRNJ9cqJqjP"
      },
      "source": [
        "set up wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "IuwYk1ySIpp8"
      },
      "outputs": [],
      "source": [
        "!pip install wandb --upgrade\n",
        "\n",
        "import wandb\n",
        "wandb.login()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nXDBvaR3JuKH"
      },
      "source": [
        "define the sweep"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YkUe7Y3FJwjE"
      },
      "outputs": [],
      "source": [
        "sweep_config = {\n",
        "    'method': 'random'\n",
        "    }\n",
        "\n",
        "metric = {\n",
        "    'name': 'loss',\n",
        "    'goal': 'minimize'\n",
        "    }\n",
        "\n",
        "sweep_config['metric'] = metric\n",
        "\n",
        "parameters_dict = {\n",
        "    'learning_rate': {\n",
        "        'distribution': 'log_uniform_values',\n",
        "        'min': 1e-5,\n",
        "        'max': 1e-3\n",
        "        },\n",
        "    'warmup_ratio': {\n",
        "        'values': [0.05, 0.1, 0.2]\n",
        "        },\n",
        "    'weight_decay': {\n",
        "        'values': [0.01, 0.03, 0.05]\n",
        "        },\n",
        "    'per_device_train_batch_size': {\n",
        "        'values': [2, 4]\n",
        "        },\n",
        "    'gradient_accumulation_steps': {\n",
        "        'values': [2, 4, 8]\n",
        "        },\n",
        "    'epochs': {\n",
        "        'value': 1\n",
        "        }\n",
        "}\n",
        "\n",
        "sweep_config['parameters'] = parameters_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cE0zTn7sRRHj"
      },
      "outputs": [],
      "source": [
        "import pprint\n",
        "\n",
        "pprint.pprint(sweep_config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z6hC7qnObJHV"
      },
      "source": [
        "Define class\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "collapsed": true,
        "id": "XZk6B1XqqQkM"
      },
      "outputs": [],
      "source": [
        "class MemoryCallback(TrainerCallback):\n",
        "    def on_step_end(self, args, state, control, **kwargs):\n",
        "        if state.global_step % 50 == 0:  # ÊØè50Ê≠•Ê∏ÖÁêÜ‰∏ÄÊ¨°\n",
        "            clear_memory()\n",
        "            print_detailed_gpu_info()\n",
        "\n",
        "class AIGenerationDetector:\n",
        "    def __init__(self, max_seq_length=2048, save_dir='/content/drive/MyDrive/ai_detection_model', data_dir='/content/drive/MyDrive/ai_dataset'):\n",
        "        \"\"\"\n",
        "        Initializes the AIGenerationDetector class.\n",
        "        Args:\n",
        "            max_seq_length (int): Maximum sequence length for the model.\n",
        "            save_dir (str): Directory to save the model and checkpoints.\n",
        "            data_dir (str): Directory to save the dataset.\n",
        "        \"\"\"\n",
        "        self.max_seq_length = max_seq_length\n",
        "        self.save_dir = save_dir\n",
        "        self.data_dir = data_dir\n",
        "        self.model = None\n",
        "        self.tokenizer = None\n",
        "        self.train_dataset = None\n",
        "        self.eval_dataset = None\n",
        "        self.test_dataset = None\n",
        "        os.makedirs(self.save_dir, exist_ok=True)\n",
        "        os.makedirs(self.data_dir, exist_ok=True)\n",
        "\n",
        "\n",
        "    def download_data(self):\n",
        "        \"\"\"\n",
        "        Downloads the dataset from Google Drive only if the files don't exist.\n",
        "        \"\"\"\n",
        "        # Download target folder path\n",
        "        download_dir = self.data_dir\n",
        "\n",
        "        # human.xlsx and ai.xlsx Google Drive download links\n",
        "        human_xlsx_url = \"https://drive.google.com/uc?export=download&id=1EXEs6bDZ8KYWN6wei17QTOFKT8TOHVbq\"\n",
        "        ai_xlsx_url = \"https://drive.google.com/uc?export=download&id=1fGU3P_xAfqHZeYQiwE382YmKiQZoWmFi\"\n",
        "\n",
        "        # File names\n",
        "        human_xlsx_filename = \"human.xlsx\"\n",
        "        ai_xlsx_filename = \"ai.xlsx\"\n",
        "\n",
        "        # Check if human.xlsx exists\n",
        "        human_xlsx_output_path = os.path.join(download_dir, human_xlsx_filename)\n",
        "        if not os.path.exists(human_xlsx_output_path):\n",
        "            print(f\"Downloading {human_xlsx_filename} from Google Drive to {download_dir}...\")\n",
        "            gdown.download(human_xlsx_url, human_xlsx_output_path, quiet=False)\n",
        "        else:\n",
        "            print(f\"{human_xlsx_filename} already exists in {download_dir}. Skipping download.\")\n",
        "\n",
        "        # Check if ai.xlsx exists\n",
        "        ai_xlsx_output_path = os.path.join(download_dir, ai_xlsx_filename)\n",
        "        if not os.path.exists(ai_xlsx_output_path):\n",
        "            print(f\"Downloading {ai_xlsx_filename} from Google Drive to {download_dir}...\")\n",
        "            gdown.download(ai_xlsx_url, ai_xlsx_output_path, quiet=False)\n",
        "        else:\n",
        "            print(f\"{ai_xlsx_filename} already exists in {download_dir}. Skipping download.\")\n",
        "\n",
        "        print(\"Data download check complete!\")\n",
        "\n",
        "    def setup_model(self):\n",
        "        \"\"\"\n",
        "        Loads the pre-trained model and tokenizer and configures the PEFT model.\n",
        "        \"\"\"\n",
        "        clear_memory()\n",
        "        print(\"Loading model...\")\n",
        "\n",
        "        try:\n",
        "            model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "                model_name=\"unsloth/Meta-Llama-3.1-8B\",\n",
        "                max_seq_length=self.max_seq_length,\n",
        "                load_in_4bit=True,\n",
        "            )\n",
        "\n",
        "            model = FastLanguageModel.get_peft_model(\n",
        "                model,\n",
        "                r=16,\n",
        "                target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
        "                lora_alpha=16,\n",
        "                lora_dropout=0.1,\n",
        "                bias=\"none\",\n",
        "                use_gradient_checkpointing=True,\n",
        "                random_state=3407,\n",
        "                use_rslora=True,\n",
        "            )\n",
        "\n",
        "            self.model = model\n",
        "            self.tokenizer = tokenizer\n",
        "            print(\"Model loaded successfully!\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading model: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    def prepare_datasets(self, max_samples=1000):\n",
        "        \"\"\"\n",
        "        Prepares the training, evaluation, and test datasets.\n",
        "        Args:\n",
        "            max_samples (int): Maximum number of training samples to use.\n",
        "        \"\"\"\n",
        "        clear_memory()\n",
        "        print(\"Preparing datasets...\")\n",
        "        try:\n",
        "            human_xlsx_path = os.path.join(self.data_dir, \"human.xlsx\")\n",
        "            ai_xlsx_path = os.path.join(self.data_dir, \"ai.xlsx\")\n",
        "\n",
        "            if not os.path.exists(human_xlsx_path) or not os.path.exists(ai_xlsx_path):\n",
        "                self.download_data()\n",
        "\n",
        "            print(f\"Loading data from {human_xlsx_path} and {ai_xlsx_path}...\")\n",
        "\n",
        "            df_human = pd.read_excel(human_xlsx_path)\n",
        "            df_human = df_human[['abstract']]\n",
        "            df_human['is_ai_generated'] = 'False'\n",
        "\n",
        "            df_ai = pd.read_excel(ai_xlsx_path)\n",
        "            df_ai = df_ai[['abstract']]\n",
        "            df_ai['is_ai_generated'] = 'True'\n",
        "\n",
        "            # merge\n",
        "            df = pd.concat([df_human, df_ai], ignore_index=True)\n",
        "\n",
        "            dataset = Dataset.from_pandas(df)\n",
        "            print(f\"Data loaded. Total samples: {len(dataset)}\")\n",
        "\n",
        "            # Take required number of samples, if specified\n",
        "            if max_samples > 0:\n",
        "                dataset = dataset.shuffle(seed=3407).select(range(min(max_samples, len(dataset))))\n",
        "\n",
        "            # Split train, val and test set\n",
        "            train_val_idx, test_idx = train_test_split(\n",
        "                range(len(dataset)),\n",
        "                test_size=0.2,\n",
        "                random_state=3407\n",
        "            )\n",
        "\n",
        "            train_idx, val_idx = train_test_split(\n",
        "                train_val_idx,\n",
        "                test_size=0.125,\n",
        "                random_state=3407\n",
        "            )\n",
        "\n",
        "            train_examples = [self.process_training_example(dataset[i]) for i in train_idx]\n",
        "            eval_examples = [self.process_training_example(dataset[i]) for i in val_idx]\n",
        "            test_examples = [dataset[i] for i in test_idx]\n",
        "\n",
        "            self.train_dataset = Dataset.from_list(train_examples)\n",
        "            self.eval_dataset = Dataset.from_list(eval_examples)\n",
        "            self.test_dataset = Dataset.from_list([{'text': item['abstract']} for item in test_examples])\n",
        "\n",
        "            del train_examples, eval_examples, dataset\n",
        "            clear_memory()\n",
        "            print(f\"Datasets prepared! Train size: {len(self.train_dataset)}, Eval size: {len(self.eval_dataset)}, Test size: {len(self.test_dataset)}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error preparing datasets: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "\n",
        "    def process_training_example(self, example):\n",
        "         \"\"\"\n",
        "         Processes a single training example to create the prompt.\n",
        "         Args:\n",
        "            example (dict): A dictionary containing the 'text' and 'is_ai_generated' fields.\n",
        "         Returns:\n",
        "            dict: A dictionary containing the processed text.\n",
        "         \"\"\"\n",
        "         text = example['abstract']\n",
        "         is_ai_generated = example['is_ai_generated']  # Get the field from the dataset\n",
        "\n",
        "         prompt = (\n",
        "            \"You are an expert in distinguishing between text written by humans and text generated by AI.\\n\\n\"\n",
        "            f\"Given Text: {text}\\n\\n\"\n",
        "            \"Based on careful analysis, is the text generated by an AI? Respond with EXACTLY 'True' or 'False'.\\n\"\n",
        "            f\"Answer: {str(is_ai_generated)}\"\n",
        "         ) + self.tokenizer.eos_token\n",
        "\n",
        "         return {\"text\": prompt}\n",
        "\n",
        "\n",
        "    def process_test_example(self, example):\n",
        "        \"\"\"\n",
        "        Processes a single test example to create the prompt.\n",
        "        Args:\n",
        "            example (dict): A dictionary containing the 'text' field.\n",
        "        Returns:\n",
        "            str: The generated prompt for testing.\n",
        "        \"\"\"\n",
        "        text = example['abstract']\n",
        "\n",
        "        prompt = (\n",
        "           \"You are an expert in distinguishing between text written by humans and text generated by AI.\\n\\n\"\n",
        "           f\"Given Text: {text}\\n\\n\"\n",
        "           \"Based on careful analysis, is the text generated by an AI? Respond with EXACTLY 'True' or 'False'.\\n\"\n",
        "        )\n",
        "        return prompt\n",
        "\n",
        "    def setup_training_args(self, config=None):\n",
        "        \"\"\"\n",
        "        Sets up training arguments, either default or for hyperparameter sweeping.\n",
        "        Args:\n",
        "           config (dict, optional): Configuration for hyperparameter sweep. Defaults to None.\n",
        "        Returns:\n",
        "           TrainingArguments: Training arguments based on the given configuration.\n",
        "        \"\"\"\n",
        "        if config is None:\n",
        "            # Default training arguments\n",
        "            return TrainingArguments(\n",
        "                output_dir=os.path.join(self.save_dir, \"checkpoints\"),\n",
        "                per_device_train_batch_size=2,\n",
        "                gradient_accumulation_steps=2,\n",
        "                warmup_ratio=0.2,\n",
        "                num_train_epochs=3,\n",
        "                learning_rate=0.0001671,\n",
        "                fp16=True,\n",
        "                logging_steps=10,\n",
        "                optim=\"adamw_torch\",\n",
        "                weight_decay=0.05,\n",
        "                lr_scheduler_type=\"cosine\",\n",
        "                seed=3407,\n",
        "                evaluation_strategy=\"steps\",\n",
        "                eval_steps=500,\n",
        "                save_strategy=\"steps\",\n",
        "                save_steps=500,\n",
        "                load_best_model_at_end=True,\n",
        "                metric_for_best_model=\"eval_loss\",\n",
        "                gradient_checkpointing=True,\n",
        "                max_grad_norm=0.3,\n",
        "                report_to=\"none\",\n",
        "                remove_unused_columns=True,\n",
        "                dataloader_pin_memory=False\n",
        "            )\n",
        "        else:\n",
        "            # Training arguments for hyperparameter sweep\n",
        "            return TrainingArguments(\n",
        "                output_dir=os.path.join(self.save_dir, \"checkpoints\"),\n",
        "                per_device_train_batch_size=config.per_device_train_batch_size,\n",
        "                gradient_accumulation_steps=config.gradient_accumulation_steps,\n",
        "                warmup_ratio=config.warmup_ratio,\n",
        "                num_train_epochs=config.epochs,\n",
        "                learning_rate=config.learning_rate,\n",
        "                fp16=True,\n",
        "                logging_steps=10,\n",
        "                optim=\"adamw_torch\",\n",
        "                weight_decay=config.weight_decay,\n",
        "                lr_scheduler_type=\"cosine\",\n",
        "                seed=3407,\n",
        "                evaluation_strategy=\"steps\",\n",
        "                eval_steps=50,\n",
        "                save_strategy=\"steps\",\n",
        "                save_steps=50,\n",
        "                load_best_model_at_end=True,\n",
        "                metric_for_best_model=\"eval_loss\",\n",
        "                gradient_checkpointing=True,\n",
        "                max_grad_norm=0.3,\n",
        "                report_to=\"wandb\",\n",
        "                remove_unused_columns=True,\n",
        "                dataloader_pin_memory=False,\n",
        "            )\n",
        "\n",
        "    def train(self):\n",
        "        \"\"\"\n",
        "        Trains the model using the SFTTrainer.\n",
        "        \"\"\"\n",
        "        clear_memory()\n",
        "        print(\"Starting training...\")\n",
        "\n",
        "        try:\n",
        "            trainer = SFTTrainer(\n",
        "                model=self.model,\n",
        "                tokenizer=self.tokenizer,\n",
        "                train_dataset=self.train_dataset,\n",
        "                eval_dataset=self.eval_dataset,\n",
        "                dataset_text_field=\"text\",\n",
        "                max_seq_length=self.max_seq_length,\n",
        "                dataset_num_proc=2,\n",
        "                packing=False,\n",
        "                args=self.setup_training_args(),\n",
        "                callbacks=[MemoryCallback()]\n",
        "            )\n",
        "\n",
        "            trainer.train()\n",
        "\n",
        "            final_save_path = os.path.join(self.save_dir, \"final_model\")\n",
        "            self.model.save_pretrained(final_save_path)\n",
        "            self.tokenizer.save_pretrained(final_save_path)\n",
        "            print(f\"Training completed! Model saved to {final_save_path}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error during training: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "\n",
        "    def sweep(self):\n",
        "       \"\"\"\n",
        "        Conducts a hyperparameter sweep using Weights & Biases.\n",
        "        \"\"\"\n",
        "       print(\"Starting sweeping...\")\n",
        "\n",
        "       with wandb.init():\n",
        "            config = wandb.config\n",
        "            training_args = self.setup_training_args(config)\n",
        "\n",
        "            try:\n",
        "                trainer = SFTTrainer(\n",
        "                    model=self.model,\n",
        "                    tokenizer=self.tokenizer,\n",
        "                    train_dataset=self.train_dataset,\n",
        "                    eval_dataset=self.eval_dataset,\n",
        "                    dataset_text_field=\"text\",\n",
        "                    max_seq_length=self.max_seq_length,\n",
        "                    dataset_num_proc=2,\n",
        "                    packing=False,\n",
        "                    args=training_args\n",
        "                    )\n",
        "                trainer.train()\n",
        "\n",
        "                final_save_path = os.path.join(self.save_dir, \"final_model\")\n",
        "                self.model.save_pretrained(final_save_path)\n",
        "                self.tokenizer.save_pretrained(final_save_path)\n",
        "                print(f\"Training completed! Model saved to {final_save_path}\")\n",
        "\n",
        "            except Exception as e:\n",
        "                  print(f\"Error during training: {str(e)}\")\n",
        "                  raise\n",
        "\n",
        "\n",
        "    def generate_predictions(self, batch_size=16):\n",
        "         \"\"\"\n",
        "        Generates predictions on the test dataset.\n",
        "        Args:\n",
        "           batch_size (int): Batch size for generating predictions.\n",
        "        Returns:\n",
        "           list: List of prediction values.\n",
        "        \"\"\"\n",
        "         clear_memory()\n",
        "         print(\"Generating predictions...\")\n",
        "\n",
        "         try:\n",
        "            FastLanguageModel.for_inference(self.model)\n",
        "            predictions = []\n",
        "            # Convert test data to a list to support batch processing\n",
        "            test_examples = list(self.test_dataset)\n",
        "            total_batches = (len(test_examples) + batch_size - 1) // batch_size\n",
        "            all_predictions = []\n",
        "\n",
        "            # Process in batches\n",
        "            for i in range(0, len(test_examples), batch_size):\n",
        "                if i % (batch_size * 10) == 0:\n",
        "                    print(f\"Processing batch {i//batch_size}/{total_batches}\")\n",
        "\n",
        "                # Get current batch samples\n",
        "                batch = test_examples[i:i + batch_size]\n",
        "                prompts = [self.process_test_example(example) for example in batch]\n",
        "\n",
        "                # Batch encoding\n",
        "                inputs = self.tokenizer(\n",
        "                    prompts,\n",
        "                    return_tensors=\"pt\",\n",
        "                    padding=True,\n",
        "                    truncation=True,\n",
        "                    max_length=self.max_seq_length\n",
        "                ).to(\"cuda\")\n",
        "\n",
        "                # Batch generation\n",
        "                with torch.inference_mode():\n",
        "                    outputs = self.model.generate(\n",
        "                        **inputs,\n",
        "                        max_new_tokens=8,  # Reduce the number of generated tokens as we only need True/False\n",
        "                        temperature=0.1,\n",
        "                        top_p=0.9,\n",
        "                        do_sample=False,    # Disable sampling for faster generation\n",
        "                        use_cache=True,\n",
        "                        pad_token_id=self.tokenizer.pad_token_id,\n",
        "                    )\n",
        "\n",
        "                input_length = inputs['input_ids'].shape[1]\n",
        "                responses = self.tokenizer.batch_decode(\n",
        "                    [output[input_length:] for output in outputs],\n",
        "                    skip_special_tokens=True\n",
        "                )\n",
        "\n",
        "                # Batch processing prediction results\n",
        "                batch_predictions = [\"true\" in response.lower() for response in responses]\n",
        "                all_predictions.extend(batch_predictions)\n",
        "\n",
        "                # Periodically clear memory\n",
        "                if i % (batch_size * 50) == 0:\n",
        "                    clear_memory()\n",
        "\n",
        "            print(f\"Total predictions: {len(all_predictions)}\")\n",
        "            assert len(all_predictions) == len(test_examples)\n",
        "\n",
        "            return all_predictions\n",
        "\n",
        "         except Exception as e:\n",
        "            print(f\"Error generating predictions: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "\n",
        "    def create_submission(self):\n",
        "        \"\"\"\n",
        "        Creates the submission file in CSV format using the generated predictions.\n",
        "        \"\"\"\n",
        "        print(\"Creating submission file...\")\n",
        "        try:\n",
        "            predictions = self.generate_predictions(batch_size=16)\n",
        "            print(f\"Generated predictions: {len(predictions)}\")\n",
        "\n",
        "            assert len(predictions) == len(self.test_dataset), \\\n",
        "                f\"Prediction count mismatch! Expected {len(self.test_dataset)}, got {len(predictions)}\"\n",
        "\n",
        "            submission_df = pd.DataFrame({\n",
        "                'ID': range(len(predictions)),\n",
        "                'is_ai_generated': predictions\n",
        "            })\n",
        "\n",
        "            print(f\"Submission DataFrame shape: {submission_df.shape}\")\n",
        "\n",
        "            submission_path = os.path.join(self.save_dir, 'submission.csv')\n",
        "            submission_df.to_csv(submission_path, index=False)\n",
        "            print(f\"Submission saved to {submission_path}\")\n",
        "\n",
        "            saved_df = pd.read_csv(submission_path)\n",
        "            print(f\"Saved file shape: {saved_df.shape}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error creating submission: {str(e)}\")\n",
        "            raise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6MyPxv-EImbL"
      },
      "source": [
        "# Hyper Parameters sweeping"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zrbq99E0Redc"
      },
      "source": [
        "## 1. Initial the sweep"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Q0-DB4jRggc"
      },
      "outputs": [],
      "source": [
        "sweep_id = wandb.sweep(sweep_config, project=\"ai_detection\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-eWethtVSJrV"
      },
      "source": [
        "## 2. Run sweep agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kjIDgdFtXnbP"
      },
      "outputs": [],
      "source": [
        "def run_sweep(data_dir):\n",
        "    \"\"\"\n",
        "    Runs the hyperparameter sweep.\n",
        "    Args:\n",
        "      data_dir(str): Path to the data directory.\n",
        "    \"\"\"\n",
        "    trainer = AIGenerationDetector(data_dir = data_dir) # set data directory path\n",
        "    trainer.download_data()\n",
        "    trainer.setup_model()\n",
        "    trainer.prepare_datasets(max_samples=1000)\n",
        "    trainer.sweep()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vkp4-r3U39H5"
      },
      "outputs": [],
      "source": [
        "wandb.agent(sweep_id, run_sweep, count = 50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DSV3PgSEZzXZ"
      },
      "source": [
        "# Get final result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kDmQ-L8OC4DQ"
      },
      "source": [
        "Before running the main function, hyper parameter in trainer.train() should be changed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "28bd6cebffcc47a4a3f2fff52b417a05",
            "9d792934fd764986bc0f8269d4c4ac28",
            "0dcab13c89e44270b2855ce55efda5f3",
            "2f33b4dc509243f0ae9e298c93294fb0",
            "61dd21a8a66a4d53b44041f2ecdd3265",
            "cf75040b037c4ad18f75cd422d011065",
            "f5227d2c9820448cb32352c21bd3fafb",
            "6b0f4dff424944689886654c46672cc7",
            "cfa8897f1b5647bf97e042212cf77101",
            "946fccead72f4fb4b14c6402fe64c585",
            "b08c0033ca9c4bc7950b98799284f0e1",
            "1409ba8eb45948c6849939d8ae90d390",
            "77a69b55c4194814a4b0c7bd05a47f4d",
            "5dee7c52d3b444ad80e4b58b7a6ac6a0",
            "639ea8d8daae4124bfe5d5712d2b4d9a",
            "0cff6f59007e4ae0824c085b0bd10938",
            "c8313503ee934458a271e2a93adb3aac",
            "75794c1a3b834b809814fa8a11637c1d",
            "08be17345e7d4f70b59fa61834eaf4c5",
            "d982b5d268634f14bded373687d608ac",
            "a714d2949112494cb952640d78f32348",
            "086a42a5074241eab2114fb1192a595e"
          ]
        },
        "id": "TFcIHpQQaEv4",
        "outputId": "b0ff9b48-e0b4-41bf-ad22-96c673a4b745"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting training pipeline...\n",
            "human.xlsx already exists in /content/drive/MyDrive/ai_dataset. Skipping download.\n",
            "ai.xlsx already exists in /content/drive/MyDrive/ai_dataset. Skipping download.\n",
            "Data download check complete!\n",
            "Loading model...\n",
            "==((====))==  Unsloth 2024.12.4: Fast Llama patching. Transformers:4.47.0.\n",
            "   \\\\   /|    GPU: Tesla T4. Max memory: 14.748 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.5.1+cu124. CUDA: 7.5. CUDA Toolkit: 12.4. Triton: 3.1.0\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.28.post3. FA2 = False]\n",
            " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Unsloth: Dropout = 0 is supported for fast patching. You are using dropout = 0.1.\n",
            "Unsloth will patch all other layers, except LoRA matrices, causing a performance hit.\n",
            "Unsloth 2024.12.4 patched 32 layers with 0 QKV layers, 0 O layers and 0 MLP layers.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model loaded successfully!\n",
            "Preparing datasets...\n",
            "Loading data from /content/drive/MyDrive/ai_dataset/human.xlsx and /content/drive/MyDrive/ai_dataset/ai.xlsx...\n",
            "Data loaded. Total samples: 50670\n",
            "Datasets prepared! Train size: 7000, Eval size: 1000, Test size: 2000\n",
            "Starting training...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "28bd6cebffcc47a4a3f2fff52b417a05",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map (num_proc=2):   0%|          | 0/7000 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1409ba8eb45948c6849939d8ae90d390",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map (num_proc=2):   0%|          | 0/1000 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
            "   \\\\   /|    Num examples = 7,000 | Num Epochs = 3\n",
            "O^O/ \\_/ \\    Batch size per device = 2 | Gradient Accumulation steps = 2\n",
            "\\        /    Total batch size = 4 | Total steps = 5,250\n",
            " \"-____-\"     Number of trainable parameters = 13,631,488\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='4894' max='5250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [4894/5250 7:39:20 < 33:25, 0.18 it/s, Epoch 2.80/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>3.503300</td>\n",
              "      <td>1.769978</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>3.349900</td>\n",
              "      <td>1.774702</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>3.281600</td>\n",
              "      <td>1.766509</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>3.183600</td>\n",
              "      <td>1.772232</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2500</td>\n",
              "      <td>3.447700</td>\n",
              "      <td>1.765805</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3000</td>\n",
              "      <td>3.034600</td>\n",
              "      <td>1.758958</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3500</td>\n",
              "      <td>3.196800</td>\n",
              "      <td>1.746989</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4000</td>\n",
              "      <td>2.952300</td>\n",
              "      <td>1.787331</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4500</td>\n",
              "      <td>3.095000</td>\n",
              "      <td>1.781427</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Detailed GPU Memory Info:\n",
            "Allocated: 6151.87 MB\n",
            "Cached: 6234.00 MB\n",
            "Max Allocated: 7464.92 MB\n",
            "\n",
            "Detailed GPU Memory Info:\n",
            "Allocated: 6151.87 MB\n",
            "Cached: 6232.00 MB\n",
            "Max Allocated: 7464.92 MB\n",
            "\n",
            "Detailed GPU Memory Info:\n",
            "Allocated: 6151.87 MB\n",
            "Cached: 6234.00 MB\n",
            "Max Allocated: 7572.13 MB\n",
            "\n",
            "Detailed GPU Memory Info:\n",
            "Allocated: 6151.87 MB\n",
            "Cached: 6232.00 MB\n",
            "Max Allocated: 7572.13 MB\n",
            "\n",
            "Detailed GPU Memory Info:\n",
            "Allocated: 6151.87 MB\n",
            "Cached: 6232.00 MB\n",
            "Max Allocated: 7572.13 MB\n",
            "\n",
            "Detailed GPU Memory Info:\n",
            "Allocated: 6151.87 MB\n",
            "Cached: 6232.00 MB\n",
            "Max Allocated: 7572.13 MB\n",
            "\n",
            "Detailed GPU Memory Info:\n",
            "Allocated: 6151.87 MB\n",
            "Cached: 6234.00 MB\n",
            "Max Allocated: 7572.13 MB\n",
            "\n",
            "Detailed GPU Memory Info:\n",
            "Allocated: 6151.86 MB\n",
            "Cached: 6232.00 MB\n",
            "Max Allocated: 7572.13 MB\n",
            "\n",
            "Detailed GPU Memory Info:\n",
            "Allocated: 6151.87 MB\n",
            "Cached: 6232.00 MB\n",
            "Max Allocated: 7572.13 MB\n",
            "\n",
            "Detailed GPU Memory Info:\n",
            "Allocated: 6151.87 MB\n",
            "Cached: 6232.00 MB\n",
            "Max Allocated: 7572.13 MB\n",
            "\n",
            "Detailed GPU Memory Info:\n",
            "Allocated: 6151.87 MB\n",
            "Cached: 6234.00 MB\n",
            "Max Allocated: 7879.98 MB\n",
            "\n",
            "Detailed GPU Memory Info:\n",
            "Allocated: 6151.87 MB\n",
            "Cached: 6234.00 MB\n",
            "Max Allocated: 7879.98 MB\n",
            "\n",
            "Detailed GPU Memory Info:\n",
            "Allocated: 6151.86 MB\n",
            "Cached: 6232.00 MB\n",
            "Max Allocated: 7879.98 MB\n",
            "\n",
            "Detailed GPU Memory Info:\n",
            "Allocated: 6151.86 MB\n",
            "Cached: 6232.00 MB\n",
            "Max Allocated: 7879.98 MB\n",
            "\n",
            "Detailed GPU Memory Info:\n",
            "Allocated: 6151.87 MB\n",
            "Cached: 6234.00 MB\n",
            "Max Allocated: 7879.98 MB\n",
            "\n",
            "Detailed GPU Memory Info:\n",
            "Allocated: 6151.87 MB\n",
            "Cached: 6232.00 MB\n",
            "Max Allocated: 7879.98 MB\n",
            "\n",
            "Detailed GPU Memory Info:\n",
            "Allocated: 6151.87 MB\n",
            "Cached: 6232.00 MB\n",
            "Max Allocated: 7879.98 MB\n",
            "\n",
            "Detailed GPU Memory Info:\n",
            "Allocated: 6151.87 MB\n",
            "Cached: 6232.00 MB\n",
            "Max Allocated: 7879.98 MB\n",
            "\n",
            "Detailed GPU Memory Info:\n",
            "Allocated: 6151.87 MB\n",
            "Cached: 6234.00 MB\n",
            "Max Allocated: 7879.98 MB\n",
            "\n",
            "Detailed GPU Memory Info:\n",
            "Allocated: 6151.87 MB\n",
            "Cached: 6232.00 MB\n",
            "Max Allocated: 7879.98 MB\n",
            "\n",
            "Detailed GPU Memory Info:\n",
            "Allocated: 6151.87 MB\n",
            "Cached: 6232.00 MB\n",
            "Max Allocated: 7879.98 MB\n",
            "\n",
            "Detailed GPU Memory Info:\n",
            "Allocated: 6151.86 MB\n",
            "Cached: 6232.00 MB\n",
            "Max Allocated: 7879.98 MB\n",
            "\n",
            "Detailed GPU Memory Info:\n",
            "Allocated: 6151.86 MB\n",
            "Cached: 6232.00 MB\n",
            "Max Allocated: 7879.98 MB\n",
            "\n",
            "Detailed GPU Memory Info:\n",
            "Allocated: 6151.86 MB\n",
            "Cached: 6232.00 MB\n",
            "Max Allocated: 7879.98 MB\n",
            "\n",
            "Detailed GPU Memory Info:\n",
            "Allocated: 6151.87 MB\n",
            "Cached: 6232.00 MB\n",
            "Max Allocated: 7879.98 MB\n",
            "\n",
            "Detailed GPU Memory Info:\n",
            "Allocated: 6151.86 MB\n",
            "Cached: 6232.00 MB\n",
            "Max Allocated: 7879.98 MB\n",
            "\n",
            "Detailed GPU Memory Info:\n",
            "Allocated: 6151.87 MB\n",
            "Cached: 6234.00 MB\n",
            "Max Allocated: 7879.98 MB\n",
            "\n",
            "Detailed GPU Memory Info:\n",
            "Allocated: 6151.86 MB\n",
            "Cached: 6232.00 MB\n",
            "Max Allocated: 7879.98 MB\n",
            "\n",
            "Detailed GPU Memory Info:\n",
            "Allocated: 6151.87 MB\n",
            "Cached: 6232.00 MB\n",
            "Max Allocated: 7879.98 MB\n",
            "\n",
            "Detailed GPU Memory Info:\n",
            "Allocated: 6151.87 MB\n",
            "Cached: 6234.00 MB\n",
            "Max Allocated: 7879.98 MB\n",
            "\n",
            "Detailed GPU Memory Info:\n",
            "Allocated: 6151.87 MB\n",
            "Cached: 6232.00 MB\n",
            "Max Allocated: 7879.98 MB\n",
            "\n",
            "Detailed GPU Memory Info:\n",
            "Allocated: 6151.87 MB\n",
            "Cached: 6232.00 MB\n",
            "Max Allocated: 7879.98 MB\n",
            "\n",
            "Detailed GPU Memory Info:\n",
            "Allocated: 6151.87 MB\n",
            "Cached: 6232.00 MB\n",
            "Max Allocated: 7879.98 MB\n",
            "\n",
            "Detailed GPU Memory Info:\n",
            "Allocated: 6151.87 MB\n",
            "Cached: 6232.00 MB\n",
            "Max Allocated: 7879.98 MB\n",
            "\n",
            "Detailed GPU Memory Info:\n",
            "Allocated: 6151.87 MB\n",
            "Cached: 6232.00 MB\n",
            "Max Allocated: 7879.98 MB\n",
            "\n",
            "Detailed GPU Memory Info:\n",
            "Allocated: 6151.87 MB\n",
            "Cached: 6232.00 MB\n",
            "Max Allocated: 7879.98 MB\n",
            "\n",
            "Detailed GPU Memory Info:\n",
            "Allocated: 6151.86 MB\n",
            "Cached: 6232.00 MB\n",
            "Max Allocated: 7879.98 MB\n",
            "\n",
            "Detailed GPU Memory Info:\n",
            "Allocated: 6151.86 MB\n",
            "Cached: 6232.00 MB\n",
            "Max Allocated: 7879.98 MB\n",
            "\n",
            "Detailed GPU Memory Info:\n",
            "Allocated: 6151.87 MB\n",
            "Cached: 6232.00 MB\n",
            "Max Allocated: 7879.98 MB\n",
            "\n",
            "Detailed GPU Memory Info:\n",
            "Allocated: 6151.88 MB\n",
            "Cached: 6232.00 MB\n",
            "Max Allocated: 7879.98 MB\n",
            "\n",
            "Detailed GPU Memory Info:\n",
            "Allocated: 6151.87 MB\n",
            "Cached: 6234.00 MB\n",
            "Max Allocated: 7879.99 MB\n",
            "\n",
            "Detailed GPU Memory Info:\n",
            "Allocated: 6151.86 MB\n",
            "Cached: 6232.00 MB\n",
            "Max Allocated: 7879.99 MB\n",
            "\n",
            "Detailed GPU Memory Info:\n",
            "Allocated: 6151.87 MB\n",
            "Cached: 6232.00 MB\n",
            "Max Allocated: 7879.99 MB\n",
            "\n",
            "Detailed GPU Memory Info:\n",
            "Allocated: 6151.87 MB\n",
            "Cached: 6232.00 MB\n",
            "Max Allocated: 7879.99 MB\n",
            "\n",
            "Detailed GPU Memory Info:\n",
            "Allocated: 6151.87 MB\n",
            "Cached: 6232.00 MB\n",
            "Max Allocated: 7879.99 MB\n",
            "\n",
            "Detailed GPU Memory Info:\n",
            "Allocated: 6151.86 MB\n",
            "Cached: 6232.00 MB\n",
            "Max Allocated: 7879.99 MB\n",
            "\n",
            "Detailed GPU Memory Info:\n",
            "Allocated: 6151.86 MB\n",
            "Cached: 6232.00 MB\n",
            "Max Allocated: 7879.99 MB\n",
            "\n",
            "Detailed GPU Memory Info:\n",
            "Allocated: 6151.86 MB\n",
            "Cached: 6232.00 MB\n",
            "Max Allocated: 7879.99 MB\n",
            "\n",
            "Detailed GPU Memory Info:\n",
            "Allocated: 6151.87 MB\n",
            "Cached: 6232.00 MB\n",
            "Max Allocated: 7879.99 MB\n",
            "\n",
            "Detailed GPU Memory Info:\n",
            "Allocated: 6151.86 MB\n",
            "Cached: 6232.00 MB\n",
            "Max Allocated: 7879.99 MB\n",
            "\n",
            "Detailed GPU Memory Info:\n",
            "Allocated: 6151.86 MB\n",
            "Cached: 6232.00 MB\n",
            "Max Allocated: 7879.99 MB\n",
            "\n",
            "Detailed GPU Memory Info:\n",
            "Allocated: 6151.87 MB\n",
            "Cached: 6232.00 MB\n",
            "Max Allocated: 7879.99 MB\n",
            "\n",
            "Detailed GPU Memory Info:\n",
            "Allocated: 6151.87 MB\n",
            "Cached: 6232.00 MB\n",
            "Max Allocated: 7879.99 MB\n",
            "\n",
            "Detailed GPU Memory Info:\n",
            "Allocated: 6151.86 MB\n",
            "Cached: 6232.00 MB\n",
            "Max Allocated: 7879.99 MB\n",
            "\n",
            "Detailed GPU Memory Info:\n",
            "Allocated: 6151.87 MB\n",
            "Cached: 6232.00 MB\n",
            "Max Allocated: 7879.99 MB\n",
            "\n",
            "Detailed GPU Memory Info:\n",
            "Allocated: 6151.86 MB\n",
            "Cached: 6232.00 MB\n",
            "Max Allocated: 7879.99 MB\n",
            "\n",
            "Detailed GPU Memory Info:\n",
            "Allocated: 6151.87 MB\n",
            "Cached: 6232.00 MB\n",
            "Max Allocated: 7879.99 MB\n",
            "\n",
            "Detailed GPU Memory Info:\n",
            "Allocated: 6151.86 MB\n",
            "Cached: 6234.00 MB\n",
            "Max Allocated: 7879.99 MB\n",
            "\n",
            "Detailed GPU Memory Info:\n",
            "Allocated: 6151.86 MB\n",
            "Cached: 6234.00 MB\n",
            "Max Allocated: 7879.99 MB\n",
            "\n",
            "Detailed GPU Memory Info:\n",
            "Allocated: 6151.86 MB\n",
            "Cached: 6232.00 MB\n",
            "Max Allocated: 7879.99 MB\n",
            "\n",
            "Detailed GPU Memory Info:\n",
            "Allocated: 6151.87 MB\n",
            "Cached: 6232.00 MB\n",
            "Max Allocated: 7879.99 MB\n",
            "\n",
            "Detailed GPU Memory Info:\n",
            "Allocated: 6151.86 MB\n",
            "Cached: 6232.00 MB\n",
            "Max Allocated: 7879.99 MB\n",
            "\n",
            "Detailed GPU Memory Info:\n",
            "Allocated: 6151.87 MB\n",
            "Cached: 6232.00 MB\n",
            "Max Allocated: 7879.99 MB\n",
            "\n",
            "Detailed GPU Memory Info:\n",
            "Allocated: 6151.86 MB\n",
            "Cached: 6232.00 MB\n",
            "Max Allocated: 7879.99 MB\n",
            "\n",
            "Detailed GPU Memory Info:\n",
            "Allocated: 6151.87 MB\n",
            "Cached: 6232.00 MB\n",
            "Max Allocated: 7879.99 MB\n",
            "\n",
            "Detailed GPU Memory Info:\n",
            "Allocated: 6151.86 MB\n",
            "Cached: 6232.00 MB\n",
            "Max Allocated: 7879.99 MB\n",
            "\n",
            "Detailed GPU Memory Info:\n",
            "Allocated: 6151.87 MB\n",
            "Cached: 6232.00 MB\n",
            "Max Allocated: 7879.99 MB\n",
            "\n",
            "Detailed GPU Memory Info:\n",
            "Allocated: 6151.87 MB\n",
            "Cached: 6232.00 MB\n",
            "Max Allocated: 7879.99 MB\n",
            "\n",
            "Detailed GPU Memory Info:\n",
            "Allocated: 6151.87 MB\n",
            "Cached: 6232.00 MB\n",
            "Max Allocated: 7879.99 MB\n",
            "\n",
            "Detailed GPU Memory Info:\n",
            "Allocated: 6151.87 MB\n",
            "Cached: 6232.00 MB\n",
            "Max Allocated: 7879.99 MB\n",
            "\n",
            "Detailed GPU Memory Info:\n",
            "Allocated: 6151.86 MB\n",
            "Cached: 6232.00 MB\n",
            "Max Allocated: 7879.99 MB\n",
            "\n",
            "Detailed GPU Memory Info:\n",
            "Allocated: 6151.86 MB\n",
            "Cached: 6234.00 MB\n",
            "Max Allocated: 7879.99 MB\n",
            "\n",
            "Detailed GPU Memory Info:\n",
            "Allocated: 6151.86 MB\n",
            "Cached: 6232.00 MB\n",
            "Max Allocated: 7879.99 MB\n",
            "\n",
            "Detailed GPU Memory Info:\n",
            "Allocated: 6151.86 MB\n",
            "Cached: 6232.00 MB\n",
            "Max Allocated: 7879.99 MB\n",
            "\n",
            "Detailed GPU Memory Info:\n",
            "Allocated: 6151.86 MB\n",
            "Cached: 6232.00 MB\n",
            "Max Allocated: 7879.99 MB\n",
            "\n",
            "Detailed GPU Memory Info:\n",
            "Allocated: 6151.87 MB\n",
            "Cached: 6232.00 MB\n",
            "Max Allocated: 7879.99 MB\n",
            "\n",
            "Detailed GPU Memory Info:\n",
            "Allocated: 6151.87 MB\n",
            "Cached: 6232.00 MB\n",
            "Max Allocated: 7879.99 MB\n",
            "\n",
            "Detailed GPU Memory Info:\n",
            "Allocated: 6151.86 MB\n",
            "Cached: 6232.00 MB\n",
            "Max Allocated: 7879.99 MB\n",
            "\n",
            "Detailed GPU Memory Info:\n",
            "Allocated: 6151.86 MB\n",
            "Cached: 6232.00 MB\n",
            "Max Allocated: 7879.99 MB\n",
            "\n",
            "Detailed GPU Memory Info:\n",
            "Allocated: 6151.87 MB\n",
            "Cached: 6232.00 MB\n",
            "Max Allocated: 7879.99 MB\n",
            "\n",
            "Detailed GPU Memory Info:\n",
            "Allocated: 6151.86 MB\n",
            "Cached: 6232.00 MB\n",
            "Max Allocated: 7879.99 MB\n",
            "\n",
            "Detailed GPU Memory Info:\n",
            "Allocated: 6151.86 MB\n",
            "Cached: 6232.00 MB\n",
            "Max Allocated: 7879.99 MB\n",
            "\n",
            "Detailed GPU Memory Info:\n",
            "Allocated: 6151.87 MB\n",
            "Cached: 6232.00 MB\n",
            "Max Allocated: 7879.99 MB\n",
            "\n",
            "Detailed GPU Memory Info:\n",
            "Allocated: 6151.86 MB\n",
            "Cached: 6232.00 MB\n",
            "Max Allocated: 7879.99 MB\n",
            "\n",
            "Detailed GPU Memory Info:\n",
            "Allocated: 6151.86 MB\n",
            "Cached: 6232.00 MB\n",
            "Max Allocated: 7879.99 MB\n",
            "\n",
            "Detailed GPU Memory Info:\n",
            "Allocated: 6151.86 MB\n",
            "Cached: 6232.00 MB\n",
            "Max Allocated: 7879.99 MB\n",
            "\n",
            "Detailed GPU Memory Info:\n",
            "Allocated: 6151.87 MB\n",
            "Cached: 6232.00 MB\n",
            "Max Allocated: 7879.99 MB\n",
            "\n",
            "Detailed GPU Memory Info:\n",
            "Allocated: 6151.87 MB\n",
            "Cached: 6232.00 MB\n",
            "Max Allocated: 7879.99 MB\n",
            "\n",
            "Detailed GPU Memory Info:\n",
            "Allocated: 6151.86 MB\n",
            "Cached: 6232.00 MB\n",
            "Max Allocated: 7879.99 MB\n",
            "\n",
            "Detailed GPU Memory Info:\n",
            "Allocated: 6151.87 MB\n",
            "Cached: 6234.00 MB\n",
            "Max Allocated: 7879.99 MB\n",
            "\n",
            "Detailed GPU Memory Info:\n",
            "Allocated: 6151.87 MB\n",
            "Cached: 6232.00 MB\n",
            "Max Allocated: 7879.99 MB\n",
            "\n",
            "Detailed GPU Memory Info:\n",
            "Allocated: 6151.86 MB\n",
            "Cached: 6232.00 MB\n",
            "Max Allocated: 7879.99 MB\n",
            "\n",
            "Detailed GPU Memory Info:\n",
            "Allocated: 6151.86 MB\n",
            "Cached: 6232.00 MB\n",
            "Max Allocated: 7879.99 MB\n",
            "\n",
            "Detailed GPU Memory Info:\n",
            "Allocated: 6151.87 MB\n",
            "Cached: 6232.00 MB\n",
            "Max Allocated: 7879.99 MB\n",
            "\n",
            "Detailed GPU Memory Info:\n",
            "Allocated: 6151.86 MB\n",
            "Cached: 6234.00 MB\n",
            "Max Allocated: 7879.99 MB\n",
            "\n",
            "Detailed GPU Memory Info:\n",
            "Allocated: 6151.86 MB\n",
            "Cached: 6234.00 MB\n",
            "Max Allocated: 7879.99 MB\n",
            "\n",
            "Detailed GPU Memory Info:\n",
            "Allocated: 6151.86 MB\n",
            "Cached: 6232.00 MB\n",
            "Max Allocated: 7879.99 MB\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='5250' max='5250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [5250/5250 8:14:29, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>3.503300</td>\n",
              "      <td>1.769978</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>3.349900</td>\n",
              "      <td>1.774702</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>3.281600</td>\n",
              "      <td>1.766509</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>3.183600</td>\n",
              "      <td>1.772232</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2500</td>\n",
              "      <td>3.447700</td>\n",
              "      <td>1.765805</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3000</td>\n",
              "      <td>3.034600</td>\n",
              "      <td>1.758958</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3500</td>\n",
              "      <td>3.196800</td>\n",
              "      <td>1.746989</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4000</td>\n",
              "      <td>2.952300</td>\n",
              "      <td>1.787331</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4500</td>\n",
              "      <td>3.095000</td>\n",
              "      <td>1.781427</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5000</td>\n",
              "      <td>2.679300</td>\n",
              "      <td>1.784756</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Detailed GPU Memory Info:\n",
            "Allocated: 6151.86 MB\n",
            "Cached: 6232.00 MB\n",
            "Max Allocated: 7879.99 MB\n",
            "\n",
            "Detailed GPU Memory Info:\n",
            "Allocated: 6151.86 MB\n",
            "Cached: 6232.00 MB\n",
            "Max Allocated: 7879.99 MB\n",
            "\n",
            "Detailed GPU Memory Info:\n",
            "Allocated: 6151.86 MB\n",
            "Cached: 6232.00 MB\n",
            "Max Allocated: 7879.99 MB\n",
            "\n",
            "Detailed GPU Memory Info:\n",
            "Allocated: 6151.86 MB\n",
            "Cached: 6232.00 MB\n",
            "Max Allocated: 7879.99 MB\n",
            "\n",
            "Detailed GPU Memory Info:\n",
            "Allocated: 6151.86 MB\n",
            "Cached: 6232.00 MB\n",
            "Max Allocated: 7879.99 MB\n",
            "\n",
            "Detailed GPU Memory Info:\n",
            "Allocated: 6151.87 MB\n",
            "Cached: 6232.00 MB\n",
            "Max Allocated: 7879.99 MB\n",
            "\n",
            "Detailed GPU Memory Info:\n",
            "Allocated: 6151.86 MB\n",
            "Cached: 6234.00 MB\n",
            "Max Allocated: 7879.99 MB\n",
            "\n",
            "Detailed GPU Memory Info:\n",
            "Allocated: 6151.86 MB\n",
            "Cached: 6232.00 MB\n",
            "Max Allocated: 7879.99 MB\n",
            "Training completed! Model saved to /content/drive/MyDrive/ai_detection_model/final_model\n",
            "Creating submission file...\n",
            "Generating predictions...\n",
            "Processing batch 0/125\n",
            "Error generating predictions: 'abstract'\n",
            "Error creating submission: 'abstract'\n",
            "Fatal error in main: 'abstract'\n",
            "\n",
            "GPU Memory Usage:\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "\n",
            "Detailed GPU Memory Info:\n",
            "Allocated: 6047.84 MB\n",
            "Cached: 6120.00 MB\n",
            "Max Allocated: 7879.99 MB\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'abstract'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-565dba772ba2>\u001b[0m in \u001b[0;36m<cell line: 30>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;31m# Run the final training using main()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-4-565dba772ba2>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(data_dir)\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare_datasets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_submission\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training pipeline completed successfully!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-80053b3602b3>\u001b[0m in \u001b[0;36mcreate_submission\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    405\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Creating submission file...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 407\u001b[0;31m             \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_predictions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    408\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Generated predictions: {len(predictions)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-80053b3602b3>\u001b[0m in \u001b[0;36mgenerate_predictions\u001b[0;34m(self, batch_size)\u001b[0m\n\u001b[1;32m    352\u001b[0m                 \u001b[0;31m# Get current batch samples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_examples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 354\u001b[0;31m                 \u001b[0mprompts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_test_example\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexample\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mexample\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    355\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m                 \u001b[0;31m# Batch encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-80053b3602b3>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    352\u001b[0m                 \u001b[0;31m# Get current batch samples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_examples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 354\u001b[0;31m                 \u001b[0mprompts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_test_example\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexample\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mexample\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    355\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m                 \u001b[0;31m# Batch encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-80053b3602b3>\u001b[0m in \u001b[0;36mprocess_test_example\u001b[0;34m(self, example)\u001b[0m\n\u001b[1;32m    187\u001b[0m             \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mThe\u001b[0m \u001b[0mgenerated\u001b[0m \u001b[0mprompt\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtesting\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m         \"\"\"\n\u001b[0;32m--> 189\u001b[0;31m         \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'abstract'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m         prompt = (\n",
            "\u001b[0;31mKeyError\u001b[0m: 'abstract'"
          ]
        }
      ],
      "source": [
        "def main(data_dir):\n",
        "    \"\"\"\n",
        "    Main training and evaluation pipeline.\n",
        "    Args:\n",
        "       data_dir (str): Path to data directory.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        set_seeds()\n",
        "        print(\"Starting training pipeline...\")\n",
        "\n",
        "        # Initialize and run trainer\n",
        "        trainer = AIGenerationDetector(data_dir=data_dir)  # set data directory path\n",
        "        trainer.download_data()\n",
        "        trainer.setup_model()\n",
        "        trainer.prepare_datasets(max_samples=10000)\n",
        "        trainer.train()\n",
        "        trainer.create_submission()\n",
        "\n",
        "        print(\"Training pipeline completed successfully!\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Fatal error in main: {str(e)}\")\n",
        "        raise\n",
        "    finally:\n",
        "        clear_memory()\n",
        "        print_gpu_utilization()\n",
        "        print_detailed_gpu_info()\n",
        "\n",
        "# Main execution block\n",
        "if __name__ == \"__main__\":\n",
        "    data_dir = \"/content/drive/MyDrive/ai_dataset\"  # Set the Google Drive path to save data\n",
        "\n",
        "    # Run the final training using main()\n",
        "    main(data_dir)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "6MyPxv-EImbL"
      ],
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "28bd6cebffcc47a4a3f2fff52b417a05": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9d792934fd764986bc0f8269d4c4ac28",
              "IPY_MODEL_0dcab13c89e44270b2855ce55efda5f3",
              "IPY_MODEL_2f33b4dc509243f0ae9e298c93294fb0"
            ],
            "layout": "IPY_MODEL_61dd21a8a66a4d53b44041f2ecdd3265"
          }
        },
        "9d792934fd764986bc0f8269d4c4ac28": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cf75040b037c4ad18f75cd422d011065",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_f5227d2c9820448cb32352c21bd3fafb",
            "value": "Map‚Äá(num_proc=2):‚Äá100%"
          }
        },
        "0dcab13c89e44270b2855ce55efda5f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6b0f4dff424944689886654c46672cc7",
            "max": 7000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_cfa8897f1b5647bf97e042212cf77101",
            "value": 7000
          }
        },
        "2f33b4dc509243f0ae9e298c93294fb0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_946fccead72f4fb4b14c6402fe64c585",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_b08c0033ca9c4bc7950b98799284f0e1",
            "value": "‚Äá7000/7000‚Äá[00:06&lt;00:00,‚Äá1485.77‚Äáexamples/s]"
          }
        },
        "61dd21a8a66a4d53b44041f2ecdd3265": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cf75040b037c4ad18f75cd422d011065": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f5227d2c9820448cb32352c21bd3fafb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6b0f4dff424944689886654c46672cc7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cfa8897f1b5647bf97e042212cf77101": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "946fccead72f4fb4b14c6402fe64c585": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b08c0033ca9c4bc7950b98799284f0e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1409ba8eb45948c6849939d8ae90d390": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_77a69b55c4194814a4b0c7bd05a47f4d",
              "IPY_MODEL_5dee7c52d3b444ad80e4b58b7a6ac6a0",
              "IPY_MODEL_639ea8d8daae4124bfe5d5712d2b4d9a"
            ],
            "layout": "IPY_MODEL_0cff6f59007e4ae0824c085b0bd10938"
          }
        },
        "77a69b55c4194814a4b0c7bd05a47f4d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c8313503ee934458a271e2a93adb3aac",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_75794c1a3b834b809814fa8a11637c1d",
            "value": "Map‚Äá(num_proc=2):‚Äá100%"
          }
        },
        "5dee7c52d3b444ad80e4b58b7a6ac6a0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_08be17345e7d4f70b59fa61834eaf4c5",
            "max": 1000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d982b5d268634f14bded373687d608ac",
            "value": 1000
          }
        },
        "639ea8d8daae4124bfe5d5712d2b4d9a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a714d2949112494cb952640d78f32348",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_086a42a5074241eab2114fb1192a595e",
            "value": "‚Äá1000/1000‚Äá[00:03&lt;00:00,‚Äá359.77‚Äáexamples/s]"
          }
        },
        "0cff6f59007e4ae0824c085b0bd10938": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c8313503ee934458a271e2a93adb3aac": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "75794c1a3b834b809814fa8a11637c1d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "08be17345e7d4f70b59fa61834eaf4c5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d982b5d268634f14bded373687d608ac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a714d2949112494cb952640d78f32348": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "086a42a5074241eab2114fb1192a595e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}