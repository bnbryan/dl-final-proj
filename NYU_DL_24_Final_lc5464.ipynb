{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "QKxfGTnfq2Sn",
        "outputId": "8b63511a-70d9-4d4b-97fb-a28ce0ee67c3"
      },
      "outputs": [],
      "source": [
        "# Uninstall potentially conflicting packages\n",
        "!pip uninstall -y transformers accelerate unsloth torch torchvision torchaudio\n",
        "\n",
        "# Install base packages\n",
        "!pip install unsloth\n",
        "\n",
        "# Install dependencies\n",
        "!pip install -q transformers accelerate peft\n",
        "!pip install -q datasets evaluate bitsandbytes trl\n",
        "!pip install -q torch torchvision torchaudio\n",
        "\n",
        "# Install Colab-optimized unsloth\n",
        "!pip uninstall unsloth -y\n",
        "!pip install --upgrade --no-cache-dir \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
        "\n",
        "# Install other tools\n",
        "!pip install pandas scikit-learn\n",
        "!pip install -q ipywidgets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uIWN2AazbCtN"
      },
      "source": [
        "Preparations\n",
        "------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "q2ADdar0bCBw",
        "outputId": "5499bfb0-b97d-4f87-d857-493ef7550c08"
      },
      "outputs": [],
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Environment setup\n",
        "import os\n",
        "import warnings\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "from datasets import load_dataset, Dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "import gc\n",
        "from unsloth import FastLanguageModel\n",
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments, TrainerCallback\n",
        "import transformers\n",
        "import accelerate\n",
        "import json\n",
        "import openpyxl\n",
        "from datasets import Dataset\n",
        "import gdown\n",
        "\n",
        "# Print versions\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"Transformers version: {transformers.__version__}\")\n",
        "print(f\"Accelerate version: {accelerate.__version__}\")\n",
        "\n",
        "# Configure environment\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
        "warnings.filterwarnings('ignore')\n",
        "torch.set_float32_matmul_precision('high')\n",
        "\n",
        "# Set random seeds\n",
        "def set_seeds(seed=3407):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "# Memory management utilities\n",
        "def clear_memory():\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "def print_gpu_utilization():\n",
        "    print(\"\\nGPU Memory Usage:\")\n",
        "    !nvidia-smi | grep -E \"Memory|Volatile\"\n",
        "\n",
        "def print_detailed_gpu_info():\n",
        "    print(\"\\nDetailed GPU Memory Info:\")\n",
        "    print(f\"Allocated: {torch.cuda.memory_allocated()/1024**2:.2f} MB\")\n",
        "    print(f\"Cached: {torch.cuda.memory_reserved()/1024**2:.2f} MB\")\n",
        "    print(f\"Max Allocated: {torch.cuda.max_memory_allocated()/1024**2:.2f} MB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bBRNJ9cqJqjP"
      },
      "source": [
        "set up wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "IuwYk1ySIpp8"
      },
      "outputs": [],
      "source": [
        "!pip install wandb --upgrade\n",
        "\n",
        "import wandb\n",
        "wandb.login()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nXDBvaR3JuKH"
      },
      "source": [
        "define the sweep"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YkUe7Y3FJwjE"
      },
      "outputs": [],
      "source": [
        "sweep_config = {\n",
        "    'method': 'random'\n",
        "    }\n",
        "\n",
        "metric = {\n",
        "    'name': 'loss',\n",
        "    'goal': 'minimize'\n",
        "    }\n",
        "\n",
        "sweep_config['metric'] = metric\n",
        "\n",
        "parameters_dict = {\n",
        "    'learning_rate': {\n",
        "        'distribution': 'log_uniform_values',\n",
        "        'min': 1e-5,\n",
        "        'max': 1e-3\n",
        "        },\n",
        "    'warmup_ratio': {\n",
        "        'values': [0.05, 0.1, 0.2]\n",
        "        },\n",
        "    'weight_decay': {\n",
        "        'values': [0.01, 0.03, 0.05]\n",
        "        },\n",
        "    'per_device_train_batch_size': {\n",
        "        'values': [2, 4]\n",
        "        },\n",
        "    'gradient_accumulation_steps': {\n",
        "        'values': [2, 4, 8]\n",
        "        },\n",
        "    'epochs': {\n",
        "        'value': 1\n",
        "        }\n",
        "}\n",
        "\n",
        "sweep_config['parameters'] = parameters_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cE0zTn7sRRHj"
      },
      "outputs": [],
      "source": [
        "import pprint\n",
        "\n",
        "pprint.pprint(sweep_config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z6hC7qnObJHV"
      },
      "source": [
        "Define class\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "collapsed": true,
        "id": "XZk6B1XqqQkM"
      },
      "outputs": [],
      "source": [
        "class MemoryCallback(TrainerCallback):\n",
        "    def on_step_end(self, args, state, control, **kwargs):\n",
        "        if state.global_step % 50 == 0:  # 每50步清理一次\n",
        "            clear_memory()\n",
        "            print_detailed_gpu_info()\n",
        "\n",
        "class AIGenerationDetector:\n",
        "    def __init__(self, max_seq_length=2048, save_dir='/content/drive/MyDrive/ai_detection_model', data_dir='/content/drive/MyDrive/ai_dataset'):\n",
        "        \"\"\"\n",
        "        Initializes the AIGenerationDetector class.\n",
        "        Args:\n",
        "            max_seq_length (int): Maximum sequence length for the model.\n",
        "            save_dir (str): Directory to save the model and checkpoints.\n",
        "            data_dir (str): Directory to save the dataset.\n",
        "        \"\"\"\n",
        "        self.max_seq_length = max_seq_length\n",
        "        self.save_dir = save_dir\n",
        "        self.data_dir = data_dir\n",
        "        self.model = None\n",
        "        self.tokenizer = None\n",
        "        self.train_dataset = None\n",
        "        self.eval_dataset = None\n",
        "        self.test_dataset = None\n",
        "        os.makedirs(self.save_dir, exist_ok=True)\n",
        "        os.makedirs(self.data_dir, exist_ok=True)\n",
        "\n",
        "\n",
        "    def download_data(self):\n",
        "        \"\"\"\n",
        "        Downloads the dataset from Google Drive only if the files don't exist.\n",
        "        \"\"\"\n",
        "        # Download target folder path\n",
        "        download_dir = self.data_dir\n",
        "\n",
        "        # human.xlsx and ai.xlsx Google Drive download links\n",
        "        human_xlsx_url = \"https://drive.google.com/uc?export=download&id=1EXEs6bDZ8KYWN6wei17QTOFKT8TOHVbq\"\n",
        "        ai_xlsx_url = \"https://drive.google.com/uc?export=download&id=1fGU3P_xAfqHZeYQiwE382YmKiQZoWmFi\"\n",
        "\n",
        "        # File names\n",
        "        human_xlsx_filename = \"human.xlsx\"\n",
        "        ai_xlsx_filename = \"ai.xlsx\"\n",
        "\n",
        "        # Check if human.xlsx exists\n",
        "        human_xlsx_output_path = os.path.join(download_dir, human_xlsx_filename)\n",
        "        if not os.path.exists(human_xlsx_output_path):\n",
        "            print(f\"Downloading {human_xlsx_filename} from Google Drive to {download_dir}...\")\n",
        "            gdown.download(human_xlsx_url, human_xlsx_output_path, quiet=False)\n",
        "        else:\n",
        "            print(f\"{human_xlsx_filename} already exists in {download_dir}. Skipping download.\")\n",
        "\n",
        "        # Check if ai.xlsx exists\n",
        "        ai_xlsx_output_path = os.path.join(download_dir, ai_xlsx_filename)\n",
        "        if not os.path.exists(ai_xlsx_output_path):\n",
        "            print(f\"Downloading {ai_xlsx_filename} from Google Drive to {download_dir}...\")\n",
        "            gdown.download(ai_xlsx_url, ai_xlsx_output_path, quiet=False)\n",
        "        else:\n",
        "            print(f\"{ai_xlsx_filename} already exists in {download_dir}. Skipping download.\")\n",
        "\n",
        "        print(\"Data download check complete!\")\n",
        "\n",
        "    def setup_model(self):\n",
        "        \"\"\"\n",
        "        Loads the pre-trained model and tokenizer and configures the PEFT model.\n",
        "        \"\"\"\n",
        "        clear_memory()\n",
        "        print(\"Loading model...\")\n",
        "\n",
        "        try:\n",
        "            model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "                model_name=\"unsloth/Meta-Llama-3.1-8B\",\n",
        "                max_seq_length=self.max_seq_length,\n",
        "                load_in_4bit=True,\n",
        "            )\n",
        "\n",
        "            model = FastLanguageModel.get_peft_model(\n",
        "                model,\n",
        "                r=16,\n",
        "                target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
        "                lora_alpha=16,\n",
        "                lora_dropout=0.1,\n",
        "                bias=\"none\",\n",
        "                use_gradient_checkpointing=True,\n",
        "                random_state=3407,\n",
        "                use_rslora=True,\n",
        "            )\n",
        "\n",
        "            self.model = model\n",
        "            self.tokenizer = tokenizer\n",
        "            print(\"Model loaded successfully!\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading model: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    def prepare_datasets(self, max_samples=1000):\n",
        "        \"\"\"\n",
        "        Prepares the training, evaluation, and test datasets.\n",
        "        Args:\n",
        "            max_samples (int): Maximum number of training samples to use.\n",
        "        \"\"\"\n",
        "        clear_memory()\n",
        "        print(\"Preparing datasets...\")\n",
        "        try:\n",
        "            human_xlsx_path = os.path.join(self.data_dir, \"human.xlsx\")\n",
        "            ai_xlsx_path = os.path.join(self.data_dir, \"ai.xlsx\")\n",
        "\n",
        "            if not os.path.exists(human_xlsx_path) or not os.path.exists(ai_xlsx_path):\n",
        "                self.download_data()\n",
        "\n",
        "            print(f\"Loading data from {human_xlsx_path} and {ai_xlsx_path}...\")\n",
        "\n",
        "            df_human = pd.read_excel(human_xlsx_path)\n",
        "            df_human = df_human[['abstract']]\n",
        "            df_human['is_ai_generated'] = 'False'\n",
        "\n",
        "            df_ai = pd.read_excel(ai_xlsx_path)\n",
        "            df_ai = df_ai[['abstract']]\n",
        "            df_ai['is_ai_generated'] = 'True'\n",
        "\n",
        "            # merge\n",
        "            df = pd.concat([df_human, df_ai], ignore_index=True)\n",
        "\n",
        "            dataset = Dataset.from_pandas(df)\n",
        "            print(f\"Data loaded. Total samples: {len(dataset)}\")\n",
        "\n",
        "            # Take required number of samples, if specified\n",
        "            if max_samples > 0:\n",
        "                dataset = dataset.shuffle(seed=3407).select(range(min(max_samples, len(dataset))))\n",
        "\n",
        "            # Split train, val and test set\n",
        "            train_val_idx, test_idx = train_test_split(\n",
        "                range(len(dataset)),\n",
        "                test_size=0.2,\n",
        "                random_state=3407\n",
        "            )\n",
        "\n",
        "            train_idx, val_idx = train_test_split(\n",
        "                train_val_idx,\n",
        "                test_size=0.125,\n",
        "                random_state=3407\n",
        "            )\n",
        "\n",
        "            train_examples = [self.process_training_example(dataset[i]) for i in train_idx]\n",
        "            eval_examples = [self.process_training_example(dataset[i]) for i in val_idx]\n",
        "            test_examples = [dataset[i] for i in test_idx]\n",
        "\n",
        "            self.train_dataset = Dataset.from_list(train_examples)\n",
        "            self.eval_dataset = Dataset.from_list(eval_examples)\n",
        "            self.test_dataset = Dataset.from_list([{'text': item['abstract']} for item in test_examples])\n",
        "\n",
        "            del train_examples, eval_examples, dataset\n",
        "            clear_memory()\n",
        "            print(f\"Datasets prepared! Train size: {len(self.train_dataset)}, Eval size: {len(self.eval_dataset)}, Test size: {len(self.test_dataset)}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error preparing datasets: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "\n",
        "    def process_training_example(self, example):\n",
        "         \"\"\"\n",
        "         Processes a single training example to create the prompt.\n",
        "         Args:\n",
        "            example (dict): A dictionary containing the 'text' and 'is_ai_generated' fields.\n",
        "         Returns:\n",
        "            dict: A dictionary containing the processed text.\n",
        "         \"\"\"\n",
        "         text = example['text']\n",
        "         is_ai_generated = example['is_ai_generated']  # Get the field from the dataset\n",
        "\n",
        "         prompt = (\n",
        "            \"You are an expert in distinguishing between text written by humans and text generated by AI.\\n\\n\"\n",
        "            f\"Given Text: {text}\\n\\n\"\n",
        "            \"Based on careful analysis, is the text generated by an AI? Respond with EXACTLY 'True' or 'False'.\\n\"\n",
        "            f\"Answer: {str(is_ai_generated)}\"\n",
        "         ) + self.tokenizer.eos_token\n",
        "\n",
        "         return {\"text\": prompt}\n",
        "\n",
        "\n",
        "    def process_test_example(self, example):\n",
        "        \"\"\"\n",
        "        Processes a single test example to create the prompt.\n",
        "        Args:\n",
        "            example (dict): A dictionary containing the 'text' field.\n",
        "        Returns:\n",
        "            str: The generated prompt for testing.\n",
        "        \"\"\"\n",
        "        text = example['text']\n",
        "\n",
        "        prompt = (\n",
        "           \"You are an expert in distinguishing between text written by humans and text generated by AI.\\n\\n\"\n",
        "           f\"Given Text: {text}\\n\\n\"\n",
        "           \"Based on careful analysis, is the text generated by an AI? Respond with EXACTLY 'True' or 'False'.\\n\"\n",
        "        )\n",
        "        return prompt\n",
        "\n",
        "    def setup_training_args(self, config=None):\n",
        "        \"\"\"\n",
        "        Sets up training arguments, either default or for hyperparameter sweeping.\n",
        "        Args:\n",
        "           config (dict, optional): Configuration for hyperparameter sweep. Defaults to None.\n",
        "        Returns:\n",
        "           TrainingArguments: Training arguments based on the given configuration.\n",
        "        \"\"\"\n",
        "        if config is None:\n",
        "            # Default training arguments\n",
        "            return TrainingArguments(\n",
        "                output_dir=os.path.join(self.save_dir, \"checkpoints\"),\n",
        "                per_device_train_batch_size=2,\n",
        "                gradient_accumulation_steps=8,\n",
        "                warmup_ratio=0.1,\n",
        "                num_train_epochs=3,\n",
        "                learning_rate=0.0006026,\n",
        "                fp16=True,\n",
        "                logging_steps=10,\n",
        "                optim=\"adamw_torch\",\n",
        "                weight_decay=0.05,\n",
        "                lr_scheduler_type=\"cosine\",\n",
        "                seed=3407,\n",
        "                evaluation_strategy=\"steps\",\n",
        "                eval_steps=50,\n",
        "                save_strategy=\"steps\",\n",
        "                save_steps=50,\n",
        "                load_best_model_at_end=True,\n",
        "                metric_for_best_model=\"eval_loss\",\n",
        "                gradient_checkpointing=True,\n",
        "                max_grad_norm=0.3,\n",
        "                report_to=\"none\",\n",
        "                remove_unused_columns=True,\n",
        "                dataloader_pin_memory=False\n",
        "            )\n",
        "        else:\n",
        "            # Training arguments for hyperparameter sweep\n",
        "            return TrainingArguments(\n",
        "                output_dir=os.path.join(self.save_dir, \"checkpoints\"),\n",
        "                per_device_train_batch_size=config.per_device_train_batch_size,\n",
        "                gradient_accumulation_steps=config.gradient_accumulation_steps,\n",
        "                warmup_ratio=config.warmup_ratio,\n",
        "                num_train_epochs=config.epochs,\n",
        "                learning_rate=config.learning_rate,\n",
        "                fp16=True,\n",
        "                logging_steps=10,\n",
        "                optim=\"adamw_torch\",\n",
        "                weight_decay=config.weight_decay,\n",
        "                lr_scheduler_type=\"cosine\",\n",
        "                seed=3407,\n",
        "                evaluation_strategy=\"steps\",\n",
        "                eval_steps=50,\n",
        "                save_strategy=\"steps\",\n",
        "                save_steps=50,\n",
        "                load_best_model_at_end=True,\n",
        "                metric_for_best_model=\"eval_loss\",\n",
        "                gradient_checkpointing=True,\n",
        "                max_grad_norm=0.3,\n",
        "                report_to=\"wandb\",\n",
        "                remove_unused_columns=True,\n",
        "                dataloader_pin_memory=False,\n",
        "            )\n",
        "\n",
        "    def train(self):\n",
        "        \"\"\"\n",
        "        Trains the model using the SFTTrainer.\n",
        "        \"\"\"\n",
        "        clear_memory()\n",
        "        print(\"Starting training...\")\n",
        "\n",
        "        try:\n",
        "            trainer = SFTTrainer(\n",
        "                model=self.model,\n",
        "                tokenizer=self.tokenizer,\n",
        "                train_dataset=self.train_dataset,\n",
        "                eval_dataset=self.eval_dataset,\n",
        "                dataset_text_field=\"text\",\n",
        "                max_seq_length=self.max_seq_length,\n",
        "                dataset_num_proc=2,\n",
        "                packing=False,\n",
        "                args=self.setup_training_args(),\n",
        "                callbacks=[MemoryCallback()]\n",
        "            )\n",
        "\n",
        "            trainer.train()\n",
        "\n",
        "            final_save_path = os.path.join(self.save_dir, \"final_model\")\n",
        "            self.model.save_pretrained(final_save_path)\n",
        "            self.tokenizer.save_pretrained(final_save_path)\n",
        "            print(f\"Training completed! Model saved to {final_save_path}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error during training: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "\n",
        "    def sweep(self):\n",
        "       \"\"\"\n",
        "        Conducts a hyperparameter sweep using Weights & Biases.\n",
        "        \"\"\"\n",
        "       print(\"Starting sweeping...\")\n",
        "\n",
        "       with wandb.init():\n",
        "            config = wandb.config\n",
        "            training_args = self.setup_training_args(config)\n",
        "\n",
        "            try:\n",
        "                trainer = SFTTrainer(\n",
        "                    model=self.model,\n",
        "                    tokenizer=self.tokenizer,\n",
        "                    train_dataset=self.train_dataset,\n",
        "                    eval_dataset=self.eval_dataset,\n",
        "                    dataset_text_field=\"text\",\n",
        "                    max_seq_length=self.max_seq_length,\n",
        "                    dataset_num_proc=2,\n",
        "                    packing=False,\n",
        "                    args=training_args\n",
        "                    )\n",
        "                trainer.train()\n",
        "\n",
        "                final_save_path = os.path.join(self.save_dir, \"final_model\")\n",
        "                self.model.save_pretrained(final_save_path)\n",
        "                self.tokenizer.save_pretrained(final_save_path)\n",
        "                print(f\"Training completed! Model saved to {final_save_path}\")\n",
        "\n",
        "            except Exception as e:\n",
        "                  print(f\"Error during training: {str(e)}\")\n",
        "                  raise\n",
        "\n",
        "\n",
        "    def generate_predictions(self, batch_size=16):\n",
        "         \"\"\"\n",
        "        Generates predictions on the test dataset.\n",
        "        Args:\n",
        "           batch_size (int): Batch size for generating predictions.\n",
        "        Returns:\n",
        "           list: List of prediction values.\n",
        "        \"\"\"\n",
        "         clear_memory()\n",
        "         print(\"Generating predictions...\")\n",
        "\n",
        "         try:\n",
        "            FastLanguageModel.for_inference(self.model)\n",
        "            predictions = []\n",
        "            # Convert test data to a list to support batch processing\n",
        "            test_examples = list(self.test_dataset)\n",
        "            total_batches = (len(test_examples) + batch_size - 1) // batch_size\n",
        "            all_predictions = []\n",
        "\n",
        "            # Process in batches\n",
        "            for i in range(0, len(test_examples), batch_size):\n",
        "                if i % (batch_size * 10) == 0:\n",
        "                    print(f\"Processing batch {i//batch_size}/{total_batches}\")\n",
        "\n",
        "                # Get current batch samples\n",
        "                batch = test_examples[i:i + batch_size]\n",
        "                prompts = [self.process_test_example(example) for example in batch]\n",
        "\n",
        "                # Batch encoding\n",
        "                inputs = self.tokenizer(\n",
        "                    prompts,\n",
        "                    return_tensors=\"pt\",\n",
        "                    padding=True,\n",
        "                    truncation=True,\n",
        "                    max_length=self.max_seq_length\n",
        "                ).to(\"cuda\")\n",
        "\n",
        "                # Batch generation\n",
        "                with torch.inference_mode():\n",
        "                    outputs = self.model.generate(\n",
        "                        **inputs,\n",
        "                        max_new_tokens=8,  # Reduce the number of generated tokens as we only need True/False\n",
        "                        temperature=0.1,\n",
        "                        top_p=0.9,\n",
        "                        do_sample=False,    # Disable sampling for faster generation\n",
        "                        use_cache=True,\n",
        "                        pad_token_id=self.tokenizer.pad_token_id,\n",
        "                    )\n",
        "\n",
        "                input_length = inputs['input_ids'].shape[1]\n",
        "                responses = self.tokenizer.batch_decode(\n",
        "                    [output[input_length:] for output in outputs],\n",
        "                    skip_special_tokens=True\n",
        "                )\n",
        "\n",
        "                # Batch processing prediction results\n",
        "                batch_predictions = [\"true\" in response.lower() for response in responses]\n",
        "                all_predictions.extend(batch_predictions)\n",
        "\n",
        "                # Periodically clear memory\n",
        "                if i % (batch_size * 50) == 0:\n",
        "                    clear_memory()\n",
        "\n",
        "            print(f\"Total predictions: {len(all_predictions)}\")\n",
        "            assert len(all_predictions) == len(test_examples)\n",
        "\n",
        "            return all_predictions\n",
        "\n",
        "         except Exception as e:\n",
        "            print(f\"Error generating predictions: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "\n",
        "    def create_submission(self):\n",
        "        \"\"\"\n",
        "        Creates the submission file in CSV format using the generated predictions.\n",
        "        \"\"\"\n",
        "        print(\"Creating submission file...\")\n",
        "        try:\n",
        "            predictions = self.generate_predictions(batch_size=16)\n",
        "            print(f\"Generated predictions: {len(predictions)}\")\n",
        "\n",
        "            assert len(predictions) == len(self.test_dataset), \\\n",
        "                f\"Prediction count mismatch! Expected {len(self.test_dataset)}, got {len(predictions)}\"\n",
        "\n",
        "            submission_df = pd.DataFrame({\n",
        "                'ID': range(len(predictions)),\n",
        "                'is_ai_generated': predictions\n",
        "            })\n",
        "\n",
        "            print(f\"Submission DataFrame shape: {submission_df.shape}\")\n",
        "\n",
        "            submission_path = os.path.join(self.save_dir, 'submission.csv')\n",
        "            submission_df.to_csv(submission_path, index=False)\n",
        "            print(f\"Submission saved to {submission_path}\")\n",
        "\n",
        "            saved_df = pd.read_csv(submission_path)\n",
        "            print(f\"Saved file shape: {saved_df.shape}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error creating submission: {str(e)}\")\n",
        "            raise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6MyPxv-EImbL"
      },
      "source": [
        "# Hyper Parameters sweeping"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zrbq99E0Redc"
      },
      "source": [
        "## 1. Initial the sweep"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Q0-DB4jRggc"
      },
      "outputs": [],
      "source": [
        "sweep_id = wandb.sweep(sweep_config, project=\"ai_detection\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-eWethtVSJrV"
      },
      "source": [
        "## 2. Run sweep agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kjIDgdFtXnbP"
      },
      "outputs": [],
      "source": [
        "def run_sweep(data_dir):\n",
        "    \"\"\"\n",
        "    Runs the hyperparameter sweep.\n",
        "    Args:\n",
        "      data_dir(str): Path to the data directory.\n",
        "    \"\"\"\n",
        "    trainer = AIGenerationDetector(data_dir = data_dir) # set data directory path\n",
        "    trainer.download_data()\n",
        "    trainer.setup_model()\n",
        "    trainer.prepare_datasets(max_samples=1000)\n",
        "    trainer.sweep()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vkp4-r3U39H5"
      },
      "outputs": [],
      "source": [
        "wandb.agent(sweep_id, run_sweep, count = 50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DSV3PgSEZzXZ"
      },
      "source": [
        "# Get final result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kDmQ-L8OC4DQ"
      },
      "source": [
        "Before running the main function, hyper parameter in trainer.train() should be changed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "cd709e55701d4312bf5e568137e98385",
            "068fc2cb1ef1460d8f9c02f9e85de4d1",
            "f5e35b2e9a5943c2a6a87fd23777685b",
            "c922b80b91314276940b0c29c45a7e38",
            "3dfeff81ccfb458195830741f7e7a30b",
            "dfa7d92c2e3042e5ad6515ec1ef1b1e5",
            "320a094e9e704f58b1b777134bd0ffdd",
            "40c90492a4dd4d499995270f1683c4a3",
            "8a6d8e3587f248859bd29d2ada1695cc",
            "a12a61d435734053ab87f611f2eb3efd",
            "f2efc5e552e845f3a67df66a2890805f",
            "010bb0d4719b423392ae023213b75129",
            "9302cb5a2d1345ae95a5fba55b68cd4c",
            "952af1eb95034fbabce6e1f92fec5b0e",
            "152467222b3a411f875c803903e47f87",
            "49c1aa545ea84337b685f8e645aee948",
            "063b4a4a972e45ebb9064ab3b442ece4",
            "de75eafc66e245b6a1d6ebabd7c622a6",
            "e6a25c883e204303aa9a37302e70df40",
            "3c2e0482e7734d8eac993af16df16168",
            "d47b9198941b44d594ff11b1a8cd0c5b",
            "5499862f9abd4a3da700da14694f8e04"
          ]
        },
        "id": "TFcIHpQQaEv4",
        "outputId": "c4b1213b-cb99-4f83-bd69-defd369a7674"
      },
      "outputs": [],
      "source": [
        "def main(data_dir):\n",
        "    \"\"\"\n",
        "    Main training and evaluation pipeline.\n",
        "    Args:\n",
        "       data_dir (str): Path to data directory.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        set_seeds()\n",
        "        print(\"Starting training pipeline...\")\n",
        "\n",
        "        # Initialize and run trainer\n",
        "        trainer = AIGenerationDetector(data_dir=data_dir)  # set data directory path\n",
        "        trainer.download_data()\n",
        "        trainer.setup_model()\n",
        "        trainer.prepare_datasets(max_samples=5000)\n",
        "        trainer.train()\n",
        "        trainer.create_submission()\n",
        "\n",
        "        print(\"Training pipeline completed successfully!\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Fatal error in main: {str(e)}\")\n",
        "        raise\n",
        "    finally:\n",
        "        clear_memory()\n",
        "        print_gpu_utilization()\n",
        "        print_detailed_gpu_info()\n",
        "\n",
        "# Main execution block\n",
        "if __name__ == \"__main__\":\n",
        "    data_dir = \"/content/drive/MyDrive/ai_dataset\"  # Set the Google Drive path to save data\n",
        "\n",
        "    # Run the final training using main()\n",
        "    main(data_dir)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "6MyPxv-EImbL"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "010bb0d4719b423392ae023213b75129": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9302cb5a2d1345ae95a5fba55b68cd4c",
              "IPY_MODEL_952af1eb95034fbabce6e1f92fec5b0e",
              "IPY_MODEL_152467222b3a411f875c803903e47f87"
            ],
            "layout": "IPY_MODEL_49c1aa545ea84337b685f8e645aee948"
          }
        },
        "063b4a4a972e45ebb9064ab3b442ece4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "068fc2cb1ef1460d8f9c02f9e85de4d1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dfa7d92c2e3042e5ad6515ec1ef1b1e5",
            "placeholder": "​",
            "style": "IPY_MODEL_320a094e9e704f58b1b777134bd0ffdd",
            "value": "Map (num_proc=2): 100%"
          }
        },
        "152467222b3a411f875c803903e47f87": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d47b9198941b44d594ff11b1a8cd0c5b",
            "placeholder": "​",
            "style": "IPY_MODEL_5499862f9abd4a3da700da14694f8e04",
            "value": " 500/500 [00:02&lt;00:00, 249.50 examples/s]"
          }
        },
        "320a094e9e704f58b1b777134bd0ffdd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3c2e0482e7734d8eac993af16df16168": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3dfeff81ccfb458195830741f7e7a30b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "40c90492a4dd4d499995270f1683c4a3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "49c1aa545ea84337b685f8e645aee948": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5499862f9abd4a3da700da14694f8e04": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8a6d8e3587f248859bd29d2ada1695cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9302cb5a2d1345ae95a5fba55b68cd4c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_063b4a4a972e45ebb9064ab3b442ece4",
            "placeholder": "​",
            "style": "IPY_MODEL_de75eafc66e245b6a1d6ebabd7c622a6",
            "value": "Map (num_proc=2): 100%"
          }
        },
        "952af1eb95034fbabce6e1f92fec5b0e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e6a25c883e204303aa9a37302e70df40",
            "max": 500,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3c2e0482e7734d8eac993af16df16168",
            "value": 500
          }
        },
        "a12a61d435734053ab87f611f2eb3efd": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c922b80b91314276940b0c29c45a7e38": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a12a61d435734053ab87f611f2eb3efd",
            "placeholder": "​",
            "style": "IPY_MODEL_f2efc5e552e845f3a67df66a2890805f",
            "value": " 4500/4500 [00:06&lt;00:00, 909.86 examples/s]"
          }
        },
        "cd709e55701d4312bf5e568137e98385": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_068fc2cb1ef1460d8f9c02f9e85de4d1",
              "IPY_MODEL_f5e35b2e9a5943c2a6a87fd23777685b",
              "IPY_MODEL_c922b80b91314276940b0c29c45a7e38"
            ],
            "layout": "IPY_MODEL_3dfeff81ccfb458195830741f7e7a30b"
          }
        },
        "d47b9198941b44d594ff11b1a8cd0c5b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "de75eafc66e245b6a1d6ebabd7c622a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dfa7d92c2e3042e5ad6515ec1ef1b1e5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e6a25c883e204303aa9a37302e70df40": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f2efc5e552e845f3a67df66a2890805f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f5e35b2e9a5943c2a6a87fd23777685b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_40c90492a4dd4d499995270f1683c4a3",
            "max": 4500,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8a6d8e3587f248859bd29d2ada1695cc",
            "value": 4500
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
